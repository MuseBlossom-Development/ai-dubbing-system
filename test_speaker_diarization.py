#!/usr/bin/env python3
"""
í™”ì ë¶„ë¦¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‚¬ìš©ë²•: python test_speaker_diarization.py <audio_file>
"""

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
import sys
import json
import argparse
from pathlib import Path

try:
    import torch
    import torchaudio
    from pyannote.audio import Pipeline
    from pydub import AudioSegment
    import numpy as np
    from collections import defaultdict
    import matplotlib.pyplot as plt

    print("âœ… í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    print(f"âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
    print("pip install pyannote.audio torch torchaudio pydub matplotlib numpy")
    sys.exit(1)


def analyze_speakers(audio_path, output_dir=None, hf_token=None):
    """í™”ì ë¶„ë¦¬ ë¶„ì„ ì‹¤í–‰"""

    if output_dir is None:
        output_dir = f"speaker_analysis_{Path(audio_path).stem}"

    os.makedirs(output_dir, exist_ok=True)

    print(f"ğŸµ ìŒì„± íŒŒì¼: {audio_path}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")

    try:
        # 1. íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        print("\nğŸ”„ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë”© ì¤‘...")

        if hf_token:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=hf_token
            )
        else:
            # í† í° ì—†ì´ ì‹œë„ (ê³µê°œ ëª¨ë¸ì¸ ê²½ìš°)
            try:
                pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1")
            except Exception as e:
                print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ Hugging Face í† í°ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                print("   https://huggingface.co/pyannote/speaker-diarization-3.1 ì—ì„œ ìŠ¹ì¸ ë°›ìœ¼ì„¸ìš”")
                return False

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPUë¡œ ì´ë™
        if torch.cuda.is_available():
            pipeline = pipeline.to(torch.device("cuda"))
            print("ğŸš€ GPU ì‚¬ìš© ì¤‘")
        else:
            print("ğŸŒ CPU ì‚¬ìš© ì¤‘ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        # 2. í™”ì ë¶„ë¦¬ ì‹¤í–‰
        print("\nğŸ” ìŒì„± ë¶„ì„ ì¤‘...")
        diarization = pipeline(audio_path)

        # 3. ê²°ê³¼ ë¶„ì„
        speakers_info = defaultdict(lambda: {'segments': [], 'duration': 0})
        timeline = []

        for turn, _, speaker in diarization.itertracks(yield_label=True):
            speakers_info[speaker]['segments'].append({
                'start': turn.start,
                'end': turn.end,
                'duration': turn.end - turn.start
            })
            speakers_info[speaker]['duration'] += turn.end - turn.start

            timeline.append({
                'start': round(turn.start, 2),
                'end': round(turn.end, 2),
                'duration': round(turn.end - turn.start, 2),
                'speaker': speaker
            })

        # ë°œí™” ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
        speakers_info = dict(sorted(speakers_info.items(),
                                    key=lambda x: x[1]['duration'], reverse=True))

        # ì‹œê°„ìˆœ ì •ë ¬
        timeline.sort(key=lambda x: x['start'])

        print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"   ì´ í™”ì ìˆ˜: {len(speakers_info)}ëª…")

        for speaker_id, info in speakers_info.items():
            print(f"   í™”ì {speaker_id}: {info['duration']:.1f}ì´ˆ ë°œí™” ({len(info['segments'])}íšŒ)")

        # 4. ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
        print(f"\nâœ‚ï¸  í™”ìë³„ ìŒì„± ì¶”ì¶œ ì¤‘...")
        audio = AudioSegment.from_file(audio_path)

        for speaker_id, info in speakers_info.items():
            speaker_dir = os.path.join(output_dir, f"speaker_{speaker_id}")
            os.makedirs(speaker_dir, exist_ok=True)

            # ê°œë³„ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
            for i, segment in enumerate(info['segments']):
                start_ms = int(segment['start'] * 1000)
                end_ms = int(segment['end'] * 1000)

                segment_audio = audio[start_ms:end_ms]
                segment_path = os.path.join(speaker_dir, f"segment_{i + 1:03d}.wav")
                segment_audio.export(segment_path, format="wav")

            # í™”ìë³„ ì „ì²´ ìŒì„± ë³‘í•©
            merged_audio = AudioSegment.empty()
            for segment in info['segments']:
                start_ms = int(segment['start'] * 1000)
                end_ms = int(segment['end'] * 1000)
                merged_audio += audio[start_ms:end_ms]

            merged_path = os.path.join(speaker_dir, f"speaker_{speaker_id}_merged.wav")
            merged_audio.export(merged_path, format="wav")

            print(f"   í™”ì {speaker_id}: {len(info['segments'])}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ")

        # 5. ë¦¬í¬íŠ¸ ìƒì„±
        print(f"\nğŸ“ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")

        # JSON ë¦¬í¬íŠ¸
        report_data = {
            'audio_file': audio_path,
            'total_speakers': len(speakers_info),
            'speakers_info': {k: {'duration': v['duration'], 'segments_count': len(v['segments'])}
                              for k, v in speakers_info.items()},
            'timeline': timeline
        }

        json_path = os.path.join(output_dir, "analysis_report.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
        txt_path = os.path.join(output_dir, "analysis_report.txt")
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("=== í™”ì ë¶„ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸ ===\n\n")
            f.write(f"ìŒì„± íŒŒì¼: {audio_path}\n")
            f.write(f"ì´ í™”ì ìˆ˜: {len(speakers_info)}ëª…\n\n")

            f.write("=== í™”ìë³„ ì •ë³´ ===\n")
            for speaker_id, info in speakers_info.items():
                f.write(f"\ní™”ì {speaker_id}:\n")
                f.write(f"  ì´ ë°œí™” ì‹œê°„: {info['duration']:.1f}ì´ˆ\n")
                f.write(f"  ë°œí™” íšŸìˆ˜: {len(info['segments'])}íšŒ\n")
                f.write(f"  í‰ê·  ë°œí™” ê¸¸ì´: {info['duration'] / len(info['segments']):.1f}ì´ˆ\n")

            f.write(f"\n=== íƒ€ì„ë¼ì¸ ({len(timeline)}ê°œ êµ¬ê°„) ===\n")
            for item in timeline:
                f.write(f"{item['start']:7.1f}s - {item['end']:7.1f}s | "
                        f"í™”ì {item['speaker']} ({item['duration']:.1f}ì´ˆ)\n")

        # 6. ì‹œê°í™” ìƒì„±
        print(f"ğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")

        try:
            # íƒ€ì„ë¼ì¸ ì‹œê°í™”
            plt.figure(figsize=(15, 8))
            speakers = list(speakers_info.keys())
            colors = plt.cm.Set3(np.linspace(0, 1, len(speakers)))
            speaker_colors = dict(zip(speakers, colors))

            y_pos = 0
            legend_added = set()

            for turn, _, speaker in diarization.itertracks(yield_label=True):
                label = f'í™”ì {speaker}' if speaker not in legend_added else ""
                if speaker not in legend_added:
                    legend_added.add(speaker)

                plt.barh(y_pos, turn.end - turn.start, left=turn.start,
                         color=speaker_colors[speaker], alpha=0.7, label=label)
                y_pos += 1

            plt.xlabel('ì‹œê°„ (ì´ˆ)')
            plt.ylabel('ë°œí™” ìˆœì„œ')
            plt.title('í™”ìë³„ ë°œí™” íƒ€ì„ë¼ì¸')
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.tight_layout()

            timeline_path = os.path.join(output_dir, "timeline.png")
            plt.savefig(timeline_path, dpi=300, bbox_inches='tight')
            plt.close()

            # ë°œí™” ì‹œê°„ ë¹„ìœ¨ íŒŒì´ì°¨íŠ¸
            plt.figure(figsize=(10, 8))
            speaker_names = [f'í™”ì {speaker}' for speaker in speakers_info.keys()]
            durations = [info['duration'] for info in speakers_info.values()]

            plt.pie(durations, labels=speaker_names, autopct='%1.1f%%', startangle=90)
            plt.title('í™”ìë³„ ë°œí™” ì‹œê°„ ë¹„ìœ¨')

            pie_path = os.path.join(output_dir, "speaker_ratio.png")
            plt.savefig(pie_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"   ì‹œê°í™” ì™„ë£Œ: timeline.png, speaker_ratio.png")

        except Exception as e:
            print(f"âš ï¸  ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")

        # 7. ì™„ë£Œ ë©”ì‹œì§€
        print(f"\nğŸ‰ í™”ì ë¶„ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ í´ë”: {output_dir}")
        print(f"ğŸ“Š ì´ {len(speakers_info)}ëª…ì˜ í™”ì ë°œê²¬")
        print(f"ğŸ“ ë¦¬í¬íŠ¸: analysis_report.txt, analysis_report.json")

        return True

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description='í™”ì ë¶„ë¦¬ í…ŒìŠ¤íŠ¸')
    parser.add_argument('audio_file', help='ë¶„ì„í•  ìŒì„±/ì˜ìƒ íŒŒì¼')
    parser.add_argument('--output', '-o', help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: speaker_analysis_<filename>)')
    parser.add_argument('--token', help='Hugging Face í† í° (í•„ìš”í•œ ê²½ìš°)')

    if len(sys.argv) == 1:
        # ì¸ìˆ˜ê°€ ì—†ìœ¼ë©´ ë„ì›€ë§ í‘œì‹œ
        parser.print_help()
        print(f"\nì˜ˆì‹œ:")
        print(f"  python {sys.argv[0]} audio.wav")
        print(f"  python {sys.argv[0]} video.mp4 --output my_analysis")
        print(f"  python {sys.argv[0]} audio.wav --token your_hf_token")
        return

    args = parser.parse_args()

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.audio_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.audio_file}")
        return

    # ë¶„ì„ ì‹¤í–‰
    success = analyze_speakers(
        audio_path=args.audio_file,
        output_dir=args.output,
        hf_token=args.token
    )

    if success:
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    else:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
