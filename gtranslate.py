# gtranslate.py

import os
import platform
import random
import re
import logging
import subprocess
from llama_cpp import Llama


def get_gpu_memory_usage():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ MB ë‹¨ìœ„ë¡œ ë°˜í™˜ (Linux/macOS/Windows ëª¨ë‘ ì§€ì›)"""
    try:
        # ì‹œìŠ¤í…œì— ë”°ë¼ ë‹¤ë¥¸ ëª…ë ¹ì–´ ì‚¬ìš©
        if platform.system() == "Linux":
            result = subprocess.check_output(
                ["nvidia-smi", "--query-gpu=memory.used", "--format=csv,nounits,noheader"],
                encoding='utf-8'
            )
        elif platform.system() == "Windows":
            result = subprocess.check_output(
                ["nvidia-smi", "-q", "-d", "MEMORY"],
                encoding='utf-8'
            )
            # Windowsì—ì„œëŠ” ì „ì²´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
            match = re.search(r'Used GPU Memory:\s+(\d+)\s+MiB', result)
            if match:
                return int(match.group(1))
            return 0

        # Linux/macOS ê²°ê³¼ ì²˜ë¦¬
        usage = int(result.strip())
        return usage

    except Exception as e:
        print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ ì‹¤íŒ¨: {e}")
        return -1


# Numba ë””ë²„ê·¸ ë¡œê·¸ ì–µì œ
logging.getLogger('numba').setLevel(logging.WARNING)
logging.getLogger('numba.core').setLevel(logging.WARNING)
logging.getLogger('numba.typed').setLevel(logging.WARNING)

# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
MODEL_PATH = os.path.join(os.path.dirname(__file__),
                          "gemma", "gemma-3-27b-it-q4_0.gguf")

# ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤
_llm = None
MAX_ATTEMPTS = 5  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€


def _get_library_path():
    """llama-cpp-python ë¹Œë“œ í›„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ë°˜í™˜"""
    system = platform.system()
    base_path = os.path.dirname(__file__)

    if system == "Windows":
        # Windows: llama-cpp-python/build/lib/Release/llama_cpp.dll
        return os.path.join(base_path, "llama-cpp-python", "build", "lib", "Release", "llama_cpp.dll")
    elif system == "Darwin":  # macOS
        # macOS: llama-cpp-python/build/lib/libllama_cpp.dylib
        return os.path.join(base_path, "llama-cpp-python", "build", "lib", "libllama_cpp.dylib")
    else:  # Linux
        # Linux: llama-cpp-python/build/lib/libllama_cpp.so
        return os.path.join(base_path, "llama-cpp-python", "build", "lib", "libllama_cpp.so")


def _get_llm():
    global _llm
    if _llm is None:
        # ë¹Œë“œëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ í™•ì¸
        library_path = _get_library_path()

        # ë¼ì´ë¸ŒëŸ¬ë¦¬ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(library_path):
            print(f"Warning: Built library not found at {library_path}")
            print("ğŸ”§ ì‹œìŠ¤í…œ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (pip ì„¤ì¹˜)")
            library_path = None  # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì‹œìŠ¤í…œ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        else:
            print(f"âœ… ì‚¬ìš©ì ë¹Œë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°œê²¬: {library_path}")

        print("ğŸ”„ LLM ì´ˆê¸°í™” ì¤‘...")
        print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {MODEL_PATH}")
        print(f"ğŸ”§ ë¼ì´ë¸ŒëŸ¬ë¦¬: {'ì‚¬ìš©ì ë¹Œë“œ' if library_path else 'ì‹œìŠ¤í…œ ê¸°ë³¸ (pip)'}")

        try:
            _llm = Llama(
                model_path=MODEL_PATH,
                n_ctx=4096,
                n_threads=4,  # GPU ì‚¬ìš© ì‹œ ìŠ¤ë ˆë“œ ìˆ˜ ì¤„ì´ê¸°
                n_gpu_layers=-1,  # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUì—ì„œ ì‹¤í–‰
                n_batch=512,  # ë°°ì¹˜ í¬ê¸° ì¡°ì •
                seed=None,  # ë§¤ë²ˆ ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš©í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
                library=library_path,
                verbose=True  # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ì„ ìœ„í•´ ë¡œê·¸ í™œì„±í™”
            )

            # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            print("âœ… LLM ì´ˆê¸°í™” ì™„ë£Œ!")

            # GPU ë ˆì´ì–´ ì •ë³´ í™•ì¸
            try:
                if hasattr(_llm, '_model'):
                    print(f"ğŸ¯ GPU ë ˆì´ì–´ ì„¤ì •: -1 (ëª¨ë“  ë ˆì´ì–´)")
                    print("ğŸ” GPU ì‚¬ìš© ì—¬ë¶€ëŠ” ì´ˆê¸°í™” ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”")
                    print("   - 'using CUDA for GPU acceleration' ë©”ì‹œì§€ ì°¾ê¸°")
                    print("   - 'VRAM used: XXX MB' ë©”ì‹œì§€ ì°¾ê¸°")
            except Exception as e:
                print(f"âš ï¸ GPU ì •ë³´ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")

        except Exception as e:
            print(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ CUDA ì§€ì› llama-cpp-python ë¹Œë“œ í•„ìš”:")
            print("   cd llama-cpp-python")
            print("   CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install -e . --verbose")
            raise

    return _llm


def _reset_llm_context():
    """LLM ì»¨í…ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ì´ì „ ë²ˆì—­ì˜ ì˜í–¥ì„ ì œê±°"""
    global _llm
    if _llm is not None:
        try:
            # llama_cppì—ì„œ ì»¨í…ìŠ¤íŠ¸ ë¦¬ì…‹ì´ ê°€ëŠ¥í•œì§€ í™•ì¸
            if hasattr(_llm, 'reset'):
                _llm.reset()
            else:
                # reset ë©”ì†Œë“œê°€ ì—†ìœ¼ë©´ ëª¨ë¸ì„ ì¬ì´ˆê¸°í™”
                print("LLM ì»¨í…ìŠ¤íŠ¸ ë¦¬ì…‹ì„ ìœ„í•´ ëª¨ë¸ ì¬ì´ˆê¸°í™”...")
                cleanup_llm()
                _llm = None
        except Exception as e:
            print(f"LLM ì»¨í…ìŠ¤íŠ¸ ë¦¬ì…‹ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëª¨ë¸ ì¬ì´ˆê¸°í™”
            cleanup_llm()
            _llm = None


def _cleanup(text: str) -> str:
    """ë²ˆí˜¸Â·ê´„í˜¸Â·ë³„í‘œ ì œê±°, ë¹ˆ ì¤„Â·ì—¬ë°± ì •ë¦¬"""
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    out = "\n".join(lines)
    out = re.sub(r'(?m)^\s*\d+\.\s*', "", out)
    out = re.sub(r'\([^)]*\)', "", out)
    out = out.replace("*", "")
    return "\n".join([ln.strip() for ln in out.split("\n") if ln.strip()])


def _contains_korean(text: str) -> bool:
    """í…ìŠ¤íŠ¸ì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê°•í™”ëœ í•¨ìˆ˜"""
    # í•œê¸€ ìëª¨, ì™„ì„±í˜• í•œê¸€, í•œê¸€ í˜¸í™˜ ìëª¨ ëª¨ë‘ ê²€ì‚¬
    korean_patterns = [
        r'[ê°€-í£]',  # ì™„ì„±í˜• í•œê¸€
        r'[ã„±-ã…]',  # í•œê¸€ ììŒ
        r'[ã…-ã…£]',  # í•œê¸€ ëª¨ìŒ
        r'[\u3130-\u318F]',  # í•œê¸€ í˜¸í™˜ ìëª¨
        r'[\uAC00-\uD7AF]',  # í•œê¸€ ìŒì ˆ
        r'[\u1100-\u11FF]',  # í•œê¸€ ìëª¨ í™•ì¥-A
        r'[\uA960-\uA97F]',  # í•œê¸€ ìëª¨ í™•ì¥-B
    ]

    for pattern in korean_patterns:
        if re.search(pattern, text):
            return True
    return False


def _remove_korean_parts(text: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ í•œê¸€ì´ í¬í•¨ëœ ë¶€ë¶„ì„ ì œê±°"""
    # í•œê¸€ì´ í¬í•¨ëœ ë‹¨ì–´ë‚˜ êµ¬ë¬¸ì„ ì œê±°
    words = text.split()
    clean_words = []

    for word in words:
        if not _contains_korean(word):
            clean_words.append(word)

    result = " ".join(clean_words).strip()

    # ì™„ì „íˆ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    if not result or result.isspace():
        return ""

    return result


def _enhanced_fallback_translate(text: str, target_lang: str) -> str:
    """ê°•í™”ëœ fallback ë²ˆì—­ í•¨ìˆ˜ (ì¼ë³¸ì–´ íŠ¹ë³„ ëŒ€ì‘)"""
    # ì¼ë³¸ì–´ìš© í™•ì¥ëœ ì‚¬ì „
    korean_japanese_dict = {
        "ë„¤": "ã¯ã„", "ì•ˆë…•í•˜ì„¸ìš”": "ã“ã‚“ã«ã¡ã¯", "ê°ì‚¬í•©ë‹ˆë‹¤": "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™",
        "ì €ëŠ”": "ç§ã¯", "ì—°ê¸°ë„": "æ¼”æŠ€ã‚‚", "í•˜ê³ ": "ã—ã¦", "ì½”ë¯¸ë””ë„": "ã‚³ãƒ¡ãƒ‡ã‚£ã‚‚",
        "í•˜ëŠ”": "ã™ã‚‹", "ì½”ë¯¸ë””ì›": "ã‚³ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ³", "ë¬¸ìƒí›ˆ": "ãƒ ãƒ³ãƒ»ã‚µãƒ³ãƒ•ãƒ³",
        "ì…ë‹ˆë‹¤": "ã§ã™", "ë§ìŠµë‹ˆë‹¤": "ãã†ã§ã™", "ì–´ë¥´ì‹ ": "ãŠå¹´å¯„ã‚Š",
        "ì €í¬": "ç§ãŸã¡", "ìƒí’ˆ": "å•†å“", "ê°œë°œ": "é–‹ç™º", "ì‰½ê²Œ": "ç°¡å˜ã«",
        "ì´í•´": "ç†è§£", "ì°¸ì—¬": "å‚åŠ ", "íŠ¹ë³„íˆ": "ç‰¹ã«", "ìˆì–´ìš”": "ã‚ã‚Šã¾ã™",
        "ê·¸ë£¹": "ã‚°ãƒ«ãƒ¼ãƒ—", "ë””ì§€í„¸": "ãƒ‡ã‚¸ã‚¿ãƒ«", "íˆ¬ì": "æŠ•è³‡", "ì „ë¬¸": "å°‚é–€",
        "ê³„ì—´ì‚¬": "é–¢é€£ä¼šç¤¾", "ë‹¹ì—°í•˜ì£ ": "å½“ç„¶ã§ã™", "í‰ìƒ": "ä¸€ç”Ÿ", "ì—´ì‹¬íˆ": "ä¸€ç”Ÿæ‡¸å‘½",
        "ëª¨ìœ¼ì‹ ": "è²¯ã‚ãŸ", "ë¶„ë“¤": "æ–¹ã€…", "ë„ˆë¬´": "ã¨ã¦ã‚‚", "ì†ìƒí•˜ì‹œì£ ": "æ‚”ã—ã„ã§ã—ã‚‡ã†"
    }

    korean_chinese_dict = {
        "ë„¤": "æ˜¯çš„", "ì•ˆë…•í•˜ì„¸ìš”": "ä½ å¥½", "ê°ì‚¬í•©ë‹ˆë‹¤": "è°¢è°¢",
        "ì €ëŠ”": "æˆ‘", "ì—°ê¸°ë„": "è¡¨æ¼”ä¹Ÿ", "í•˜ê³ ": "åš", "ì½”ë¯¸ë””ë„": "å–œå‰§ä¹Ÿ",
        "í•˜ëŠ”": "åš", "ì½”ë¯¸ë””ì›": "å–œå‰§æ¼”å‘˜", "ë¬¸ìƒí›ˆ": "æ–‡å°šå‹‹",
        "ì…ë‹ˆë‹¤": "æ˜¯", "ë§ìŠµë‹ˆë‹¤": "å¯¹", "ì–´ë¥´ì‹ ": "è€äººå®¶",
        "ì €í¬": "æˆ‘ä»¬", "ìƒí’ˆ": "äº§å“", "ê°œë°œ": "å¼€å‘", "ì‰½ê²Œ": "å®¹æ˜“",
        "ì´í•´": "ç†è§£", "ì°¸ì—¬": "å‚ä¸", "íŠ¹ë³„íˆ": "ç‰¹åˆ«", "ìˆì–´ìš”": "æœ‰",
        "ê·¸ë£¹": "é›†å›¢", "ë””ì§€í„¸": "æ•°å­—", "íˆ¬ì": "æŠ•èµ„", "ì „ë¬¸": "ä¸“ä¸š",
        "ê³„ì—´ì‚¬": "å…³è”å…¬å¸", "ë‹¹ì—°í•˜ì£ ": "å½“ç„¶", "í‰ìƒ": "ä¸€ç”Ÿ", "ì—´ì‹¬íˆ": "åŠªåŠ›",
        "ëª¨ìœ¼ì‹ ": "ç§¯æ”’", "ë¶„ë“¤": "äººä»¬", "ë„ˆë¬´": "å¤ª", "ì†ìƒí•˜ì‹œì£ ": "å¾ˆéš¾è¿‡å§"
    }

    korean_english_dict = {
        "ë„¤": "Yes", "ì•ˆë…•í•˜ì„¸ìš”": "Hello", "ê°ì‚¬í•©ë‹ˆë‹¤": "Thank you",
        "ì €ëŠ”": "I am", "ì—°ê¸°ë„": "acting too", "í•˜ê³ ": "and", "ì½”ë¯¸ë””ë„": "comedy too",
        "í•˜ëŠ”": "doing", "ì½”ë¯¸ë””ì›": "comedian", "ë¬¸ìƒí›ˆ": "Moon Sang-hun",
        "ì…ë‹ˆë‹¤": "am", "ë§ìŠµë‹ˆë‹¤": "correct", "ì–´ë¥´ì‹ ": "elders",
        "ì €í¬": "we", "ìƒí’ˆ": "product", "ê°œë°œ": "developed", "ì‰½ê²Œ": "easily",
        "ì´í•´": "understand", "ì°¸ì—¬": "participate", "íŠ¹ë³„íˆ": "specially", "ìˆì–´ìš”": "have",
        "ê·¸ë£¹": "Group", "ë””ì§€í„¸": "digital", "íˆ¬ì": "investment", "ì „ë¬¸": "specialized",
        "ê³„ì—´ì‚¬": "affiliate", "ë‹¹ì—°í•˜ì£ ": "of course", "í‰ìƒ": "lifetime", "ì—´ì‹¬íˆ": "diligently",
        "ëª¨ìœ¼ì‹ ": "saved", "ë¶„ë“¤": "people", "ë„ˆë¬´": "so", "ì†ìƒí•˜ì‹œì£ ": "upset"
    }

    # ì–¸ì–´ë³„ ì‚¬ì „ ì„ íƒ
    if target_lang == "japanese":
        trans_dict = korean_japanese_dict
        default_response = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç¿»è¨³ã§ãã¾ã›ã‚“ã§ã—ãŸ"
    elif target_lang == "chinese":
        trans_dict = korean_chinese_dict
        default_response = "æŠ±æ­‰ï¼Œæ— æ³•ç¿»è¯‘"
    else:  # english
        trans_dict = korean_english_dict
        default_response = "Translation unavailable"

    try:
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë²ˆì—­ ì‹œë„
        translated_parts = []

        # ê³µë°±ìœ¼ë¡œ ë‹¨ì–´ ë¶„ë¦¬
        words = text.split()
        for word in words:
            # íŠ¹ìˆ˜ë¬¸ì ì œê±°í•œ clean_word
            clean_word = re.sub(r'[^\wê°€-í£]', '', word)
            translated = None

            # ì‚¬ì „ì—ì„œ ê°€ì¥ ê¸´ ë§¤ì¹˜ ì°¾ê¸°
            for korean, translation in sorted(trans_dict.items(), key=len, reverse=True):
                if korean in clean_word or clean_word in korean:
                    translated = translation
                    break

            if translated:
                translated_parts.append(translated)
            else:
                # ë²ˆì—­ë˜ì§€ ì•Šì€ ë‹¨ì–´ëŠ” ê±´ë„ˆë›°ê¸°
                continue

        if translated_parts:
            result = " ".join(translated_parts)

            # ì¼ë³¸ì–´ì˜ ê²½ìš° ë¬¸ì¥ êµ¬ì¡° ê°œì„ 
            if target_lang == "japanese":
                # ê¸°ë³¸ì ì¸ ì¼ë³¸ì–´ ë¬¸ì¥ êµ¬ì¡° ê°œì„ 
                if "ç§ã¯" in result and "ã§ã™" in result:
                    # "ç§ã¯ X ã§ã™" í˜•íƒœë¡œ ì •ë¦¬
                    result = re.sub(r'ç§ã¯\s*(.*?)\s*ã§ã™', r'ç§ã¯\1ã§ã™', result)
                # ì¤‘ë³µ ì œê±°
                result = re.sub(r'\b(\w+)\s+\1\b', r'\1', result)

            return result.strip()

    except Exception as e:
        print(f"ê°•í™”ëœ fallback ë²ˆì—­ ì˜¤ë¥˜: {e}")

    return default_response


def cleanup_llm():
    """LLM ê°ì²´ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•˜ëŠ” í•¨ìˆ˜"""
    global _llm

    if _llm is not None:
        try:
            # llama_cppì˜ Llama ê°ì²´ í•´ì œ
            if hasattr(_llm, 'close'):
                _llm.close()
            elif hasattr(_llm, '__del__'):
                _llm.__del__()
        except Exception as e:
            print(f"LLM ê°ì²´ í•´ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            _llm = None

    # ê°€ë¹„ì§€ ì½œë ‰ì…˜
    import gc
    gc.collect()

    print("LLM ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")


# ì§€ì› ì–¸ì–´ ì„¤ì • - Stop wordsì— í•œê¸€ ë¬¸ì ì¶”ê°€
SUPPORTED_LANGUAGES = {
    'english': {
        'name': 'English',
        'code': 'en',
        'prompt_name': 'English',
        'stop_words': ["\n\n", "Korean:", "English:", "ê°€", "ë‚˜", "ë‹¤", "ë¼", "ë§ˆ", "ë°”", "ì‚¬", "ì•„", "ì", "ì°¨", "ì¹´", "íƒ€", "íŒŒ",
                       "í•˜"]
    },
    'chinese': {
        'name': 'Chinese',
        'code': 'zh',
        'prompt_name': 'Chinese (Simplified)',
        'stop_words': ["\n\n", "Korean:", "Chinese:", "ê°€", "ë‚˜", "ë‹¤", "ë¼", "ë§ˆ", "ë°”", "ì‚¬", "ì•„", "ì", "ì°¨", "ì¹´", "íƒ€", "íŒŒ",
                       "í•˜"],
        'length_multiplier': 1.2
    },
    'japanese': {
        'name': 'Japanese',
        'code': 'ja',
        'prompt_name': 'Japanese',
        'stop_words': ["\n\n", "Korean:", "Japanese:", "ê°€", "ë‚˜", "ë‹¤", "ë¼", "ë§ˆ", "ë°”", "ì‚¬", "ì•„", "ì", "ì°¨", "ì¹´", "íƒ€", "íŒŒ",
                       "í•˜"]
    }
}


def _create_enhanced_prompt(text: str, target_language: str, length_guide: str, is_free: bool = False) -> str:
    """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„± - í•œê¸€ ì¶œë ¥ ë°©ì§€ ê°•í™”"""

    # ê°íƒ„ì‚¬ë‚˜ ì˜ì„±ì–´ ê°ì§€
    simple_expressions = ["ë„¤", "ì˜ˆ", "ì•„", "ì˜¤", "ì–´", "ìŒ", "ì‘", "ì•„ë‹ˆ", "ê·¸ë˜", "ë§ì•„", "ì¢‹ì•„", "ì•ˆë…•"]
    is_simple = any(expr in text for expr in simple_expressions) and len(text.strip()) <= 10

    # ê°ì • í‘œí˜„ ê°ì§€
    emotion_words = ["ë†€ë", "ê¹œì§", "ê¸°ë»", "ìŠ¬í¼", "í™”ë‚˜", "ë¬´ì„œì›Œ", "ì¢‹ì•„", "ì‹«ì–´"]
    has_emotion = any(word in text for word in emotion_words)

    if is_simple or has_emotion:
        # ê°„ë‹¨í•œ í‘œí˜„ì´ë‚˜ ê°ì • í‘œí˜„ì€ ë” êµ¬ì²´ì ì¸ ê°€ì´ë“œ ì œê³µ
        base_prompt = (
            f"You are a professional translator. Translate the given Korean expression to {target_language}. "
            f"IMPORTANT: Never output Korean characters. Only provide the {target_language} translation. "
            f"If the Korean text expresses surprise, use appropriate surprise expressions in {target_language}. "
            f"If it's a simple response like 'ë„¤/ì˜ˆ', translate to appropriate response words. "
            f"{length_guide}\n\n"
            f"Korean expression: {text}\n"
            f"Translation in {target_language}:"
        )
    else:
        # ì¼ë°˜ ë¬¸ì¥
        translation_style = "with natural expressions" if is_free else "accurately"
        base_prompt = (
            f"You are a professional translator. Translate the Korean text to {target_language} {translation_style}. "
            f"CRITICAL RULE: Absolutely no Korean characters in your response. "
            f"Only output the {target_language} translation. "
            f"{length_guide}\n\n"
            f"Korean: {text}\n"
            f"{target_language} translation:"
        )

    return base_prompt


def literal_translate(text: str, max_length_ratio: float = 1.0, quality_mode: str = "balanced",
                      target_lang: str = "english") -> str:
    """
    ì§ì—­: í•œê¸€ ì¶œë ¥ ë°©ì§€ë¥¼ ê°•í™”í•œ ë²ˆì—­
    """
    if target_lang not in SUPPORTED_LANGUAGES:
        target_lang = "english"

    lang_config = SUPPORTED_LANGUAGES[target_lang]
    target_language = lang_config['name']
    stop_words = lang_config['stop_words']

    # ê¸¸ì´ ê°€ì´ë“œ ë‹¨ìˆœí™”
    if max_length_ratio < 0.8:
        length_guide = "Keep the translation concise and brief."
    else:
        length_guide = "Make the translation natural and fluent."

    # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    prompt = _create_enhanced_prompt(text, target_language, length_guide, False)

    llm = _get_llm()

    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ (ë²ˆì—­ ì‹œì‘ ì „)
    gpu_memory_before = get_gpu_memory_usage()
    if gpu_memory_before > 0:
        print(f"ğŸ”‹ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ë²ˆì—­ ì „): {gpu_memory_before} MB")

    # ë” ì—„ê²©í•œ ë§¤ê°œë³€ìˆ˜ë¡œ ì²« ë²ˆì§¸ ì‹œë„
    resp = llm(
        prompt,
        max_tokens=128,  # í† í° ìˆ˜ ì¤„ì—¬ì„œ í•œê¸€ ì¶œë ¥ ê°€ëŠ¥ì„± ê°ì†Œ
        temperature=0.1,  # ì˜¨ë„ ë” ë‚®ì¶¤
        top_p=0.8,
        stop=stop_words,
        repeat_penalty=1.1  # ë°˜ë³µ ë°©ì§€
    )
    result = resp["choices"][0]["text"].strip().strip('"\'')
    cleaned = _cleanup(result)

    # ë²ˆì—­ ê²°ê³¼ ê²€ì¦
    if cleaned.strip() and len(cleaned.strip()) > 0 and not _contains_korean(cleaned):
        print(f"[ì„±ê³µ] ì§ì—­ ì™„ë£Œ: {text} â†’ {cleaned}")
        return cleaned

    # ì¬ì‹œë„ (ë” ê°•ë ¥í•œ í”„ë¡¬í”„íŠ¸)
    print(f"[ì¬ì‹œë„] ë” ê°•ë ¥í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„: {result[:30]}...")
    _reset_llm_context()

    # ë” ê°•ë ¥í•œ í”„ë¡¬í”„íŠ¸
    strong_prompt = (
        f"TASK: Korean to {target_language} translation\n"
        f"RULE: Absolutely NO Korean characters in output\n"
        f"INPUT: {text}\n"
        f"OUTPUT ({target_language} only):"
    )

    resp = llm(
        strong_prompt,
        max_tokens=64,  # ë” ì§§ê²Œ
        temperature=0.05,  # ê±°ì˜ ê²°ì •ì 
        top_p=0.7,
        stop=stop_words,
        repeat_penalty=1.2
    )
    result = resp["choices"][0]["text"].strip().strip('"\'')
    cleaned = _cleanup(result)

    if cleaned.strip() and not _contains_korean(cleaned):
        print(f"[ì¬ì‹œë„ ì„±ê³µ] ì§ì—­ ì™„ë£Œ: {cleaned}")
        return cleaned

    # ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬ì „ ê¸°ë°˜ ë²ˆì—­ ì‹œë„
    print(f"[ì‚¬ì „ ë²ˆì—­] Gemma-3 ì‹¤íŒ¨, ì‚¬ì „ ê¸°ë°˜ ë²ˆì—­ ì‚¬ìš©: {text}")
    return _enhanced_fallback_translate(text, target_lang)


def free_translate(text: str, max_length_ratio: float = 1.0, quality_mode: str = "balanced",
                   target_lang: str = "english") -> str:
    """
    ì˜ì—­: Gemma-3 ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹ ë¢°í•˜ê³  ë‹¨ìˆœí™”
    """
    if target_lang not in SUPPORTED_LANGUAGES:
        target_lang = "english"

    lang_config = SUPPORTED_LANGUAGES[target_lang]
    target_language = lang_config['name']
    stop_words = lang_config['stop_words']

    # ì˜ì„±ì–´/ê°íƒ„ì‚¬ë§Œ ìˆìœ¼ë©´ ì§ì—­ ì‚¬ìš©
    orig_lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    if orig_lines and all(re.fullmatch(r"[ê°€-í£]+[\.!?â€¦]*", ln) for ln in orig_lines):
        return literal_translate(text, max_length_ratio, quality_mode, target_lang)

    # ê¸¸ì´ ê°€ì´ë“œ ë‹¨ìˆœí™”
    if max_length_ratio < 0.8:
        length_guide = "Keep the translation concise and brief."
    else:
        length_guide = "Make the translation natural and fluent."

    # í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    prompt = _create_enhanced_prompt(text, target_language, length_guide, True)

    llm = _get_llm()

    # ì²« ë²ˆì§¸ ì‹œë„
    resp = llm(
        prompt,
        max_tokens=256,
        temperature=0.4,  # ì˜ì—­ì€ ì¡°ê¸ˆ ë” creativeí•˜ê²Œ
        top_p=0.9,
        stop=stop_words
    )
    result = resp["choices"][0]["text"].strip().strip('"\'')
    cleaned = _cleanup(result)

    # ê²°ê³¼ ê²€ì¦: í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ì‹¤íŒ¨
    if cleaned.strip() and not _contains_korean(cleaned):
        # í•œ ì¤„ ëŒ€ë³¸ì´ë©´ ì²« ë¬¸ì¥ë§Œ
        if len(orig_lines) == 1:
            return cleaned.split("\n", 1)[0]
        return cleaned

    # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜ ê²°ê³¼ê°€ ë¶€ì ì ˆí•œ ê²½ìš° ì¬ì‹œë„
    print(f"[ì¬ì‹œë„] ì˜ì—­ ì²« ë²ˆì§¸ ê²°ê³¼ ë¶€ì ì ˆ (í•œê¸€ í¬í•¨): {result[:50]}...")
    _reset_llm_context()

    resp = llm(
        prompt,
        max_tokens=256,
        temperature=0.5,  # ì˜¨ë„ ë” ë†’ì„
        top_p=0.9,
        stop=stop_words
    )
    result = resp["choices"][0]["text"].strip().strip('"\'')
    cleaned = _cleanup(result)

    if cleaned.strip() and not _contains_korean(cleaned):
        if len(orig_lines) == 1:
            return cleaned.split("\n", 1)[0]
        return cleaned

    # ì˜ì—­ ì‹¤íŒ¨ ì‹œ ì§ì—­ìœ¼ë¡œ ëŒ€ì²´ (fallback ëŒ€ì‹ )
    print(f"[ì§ì—­ ëŒ€ì²´] ì˜ì—­ ì‹¤íŒ¨ (í•œê¸€ í¬í•¨), ì§ì—­ ì‚¬ìš©: {text}")
    return literal_translate(text, max_length_ratio, quality_mode, target_lang)


# í¸ì˜ í•¨ìˆ˜ë“¤ ì¶”ê°€
def translate_to_chinese(text: str, translation_type: str = "literal", max_length_ratio: float = 1.0,
                         quality_mode: str = "balanced") -> str:
    """ì¤‘êµ­ì–´ ë²ˆì—­ í¸ì˜ í•¨ìˆ˜"""
    if translation_type == "free":
        return free_translate(text, max_length_ratio, quality_mode, "chinese")
    else:
        return literal_translate(text, max_length_ratio, quality_mode, "chinese")


def translate_to_japanese(text: str, translation_type: str = "literal", max_length_ratio: float = 1.0,
                          quality_mode: str = "balanced") -> str:
    """ì¼ë³¸ì–´ ë²ˆì—­ í¸ì˜ í•¨ìˆ˜"""
    if translation_type == "free":
        return free_translate(text, max_length_ratio, quality_mode, "japanese")
    else:
        return literal_translate(text, max_length_ratio, quality_mode, "japanese")


def translate_to_english(text: str, translation_type: str = "literal", max_length_ratio: float = 1.0,
                         quality_mode: str = "balanced") -> str:
    """ì˜ì–´ ë²ˆì—­ í¸ì˜ í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    if translation_type == "free":
        return free_translate(text, max_length_ratio, quality_mode, "english")
    else:
        return literal_translate(text, max_length_ratio, quality_mode, "english")
