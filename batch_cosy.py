import os

# PyTorch ë””ë°”ì´ìŠ¤ ì„¤ì • (CPU/CUDAë§Œ, MPS ì œì™¸)
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import argparse
import torchaudio
import torch
import random
import numpy as np
import sys
import logging
import librosa

# íŒŒì¼ëª… ì•ˆì „í™” í•¨ìˆ˜ ì„í¬íŠ¸
from audio_processor import sanitize_filename, safe_file_operations

# í”„ë¡œì íŠ¸ ë‚´ CosyVoice2 ëª¨ë¸ ë¡œì»¬ ê²½ë¡œ ì„¤ì •
repo_root = os.path.dirname(__file__)
LOCAL_COSYVOICE_MODEL = os.path.join(
    repo_root, 'CosyVoice', 'pretrained_models', 'CosyVoice2-0.5B'
)

# CosyVoice2 íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(repo_root, 'CosyVoice'))
from cosyvoice.cli.cosyvoice import CosyVoice2

# ë¡œê¹… ì„¤ì • (Gradio ì•±ê³¼ ë™ì¼)
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


# Device-aware memory cleanup utility
def cleanup_memory(device):
    """Clean up memory for the current device"""
    if device.type == 'cuda':
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        logging.debug("CUDA cache cleared")


def cleanup_cosyvoice_model():
    """CosyVoice ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ"""
    global cosy
    if 'cosy' in globals() and cosy is not None:
        try:
            # ëª¨ë¸ êµ¬ì„± ìš”ì†Œë“¤ ë©”ëª¨ë¦¬ í•´ì œ
            if hasattr(cosy, 'model') and cosy.model is not None:
                # LLM ëª¨ë¸ í•´ì œ
                if hasattr(cosy.model, 'llm') and cosy.model.llm is not None:
                    del cosy.model.llm

                # Flow ëª¨ë¸ í•´ì œ
                if hasattr(cosy.model, 'flow') and cosy.model.flow is not None:
                    del cosy.model.flow

                # Hift ëª¨ë¸ í•´ì œ
                if hasattr(cosy.model, 'hift') and cosy.model.hift is not None:
                    del cosy.model.hift

                del cosy.model

            # Frontend í•´ì œ
            if hasattr(cosy, 'frontend') and cosy.frontend is not None:
                # ONNX ì„¸ì…˜ í•´ì œ
                if hasattr(cosy.frontend, 'campplus_session') and cosy.frontend.campplus_session is not None:
                    del cosy.frontend.campplus_session

                if hasattr(cosy.frontend,
                           'speech_tokenizer_session') and cosy.frontend.speech_tokenizer_session is not None:
                    del cosy.frontend.speech_tokenizer_session

                del cosy.frontend

            # CosyVoice ê°ì²´ í•´ì œ
            del cosy
            cosy = None

            logging.info("âœ… CosyVoice ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")

        except Exception as e:
            logging.error(f"âš ï¸ CosyVoice ëª¨ë¸ í•´ì œ ì¤‘ ì˜¤ë¥˜: {e}")

        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë° CUDA ìºì‹œ ì •ë¦¬
        import gc
        gc.collect()

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            logging.info("ğŸ”§ CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")


def synthesize_with_speed_control(cosy, text, prompt_text, prompt_wav, target_speed=1.0,
                                  target_language='korean', instruct_command=None):
    """
    CosyVoice2ì˜ ë„¤ì´í‹°ë¸Œ speed íŒŒë¼ë¯¸í„°ë¥¼ í™œìš©í•œ ìŒì„± í•©ì„±

    Args:
        cosy: CosyVoice2 ëª¨ë¸
        text: í•©ì„±í•  í…ìŠ¤íŠ¸
        prompt_text: í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
        prompt_wav: í”„ë¡¬í”„íŠ¸ ì˜¤ë””ì˜¤
        target_speed: ëª©í‘œ ì†ë„ (0.5 = ë§¤ìš°ëŠë¦¼, 1.0 = ë³´í†µ, 1.5 = ë¹ ë¦„, 2.0 = ë§¤ìš°ë¹ ë¦„)
        target_language: íƒ€ê²Ÿ ì–¸ì–´
        instruct_command: ì‚¬ìš©ì ì •ì˜ instruct ëª…ë ¹ì–´ (ë³´ì¡°ì  ì‚¬ìš©)

    Returns:
        í•©ì„±ëœ ì˜¤ë””ì˜¤ í…ì„œ
    """
    logging.info(f"  ğŸµ ë„¤ì´í‹°ë¸Œ ì†ë„ ì œì–´ í•©ì„±: {target_speed:.1f}ë°° ì†ë„")

    # 1ë‹¨ê³„: Zero-shot with native speed control (ì£¼ ë°©ì‹)
    try:
        results = cosy.inference_zero_shot(
            text,
            prompt_text,
            prompt_wav,
            "",
            stream=True,
            speed=target_speed,  # CosyVoice2ì˜ ë„¤ì´í‹°ë¸Œ speed íŒŒë¼ë¯¸í„° í™œìš©
            text_frontend=True
        )

        if results is None:
            logging.error("  âŒ Zero-shot ì†ë„ ì œì–´ í•©ì„± ì‹¤íŒ¨")
            return None

        # ê²°ê³¼ ì—°ê²°
        result_list = list(results)
        if not result_list:
            logging.error("  âŒ Zero-shot ì†ë„ ì œì–´ í•©ì„± ê²°ê³¼ ì—†ìŒ")
            return None

        combined_audio = []
        for out in result_list:
            if 'tts_speech' in out:
                speech = out['tts_speech']
                if speech.device.type != 'cpu':
                    speech = speech.cpu()
                combined_audio.append(speech)

        if not combined_audio:
            logging.error("  âŒ ìœ íš¨í•œ Zero-shot ì†ë„ ì œì–´ í•©ì„± ê²°ê³¼ ì—†ìŒ")
            return None

        final_audio = torch.cat(combined_audio, dim=1)
        duration = final_audio.size(1) / 24000
        logging.info(f"  âœ… Zero-shot ë„¤ì´í‹°ë¸Œ ì†ë„ ì œì–´ ì™„ë£Œ: {duration:.2f}ì´ˆ")

        return final_audio

    except Exception as e:
        logging.warning(f"  âš ï¸ Zero-shot ë„¤ì´í‹°ë¸Œ ì†ë„ ì œì–´ ì‹¤íŒ¨, Instruct2ë¡œ ëŒ€ì²´: {e}")

    # 2ë‹¨ê³„: Instruct2 with speed control (ëŒ€ì²´ ë°©ì‹)
    if instruct_command is None:
        if target_speed <= 0.7:
            base_command = 'ë§¤ìš° ì²œì²œíˆ ë§í•´'
        elif target_speed <= 0.9:
            base_command = 'ì²œì²œíˆ ë§í•´'
        elif target_speed >= 1.3:
            base_command = 'ë¹ ë¥´ê²Œ ë§í•´'
        elif target_speed >= 1.6:
            base_command = 'ë§¤ìš° ë¹ ë¥´ê²Œ ë§í•´'
        else:
            base_command = 'ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´'

        instruct_command = get_language_specific_instruct_command(base_command, target_language)

    logging.info(f"  ğŸ’¬ ëŒ€ì²´ Instruct ëª…ë ¹ì–´: '{instruct_command}'")

    try:
        results = cosy.inference_instruct2(
            text,
            instruct_command,
            prompt_wav,
            stream=True,
            speed=target_speed  # Instruct2ì—ì„œë„ ë„¤ì´í‹°ë¸Œ speed íŒŒë¼ë¯¸í„° í•¨ê»˜ ì‚¬ìš©
        )

        if results is None:
            logging.error("  âŒ Instruct2 ì†ë„ ì œì–´ í•©ì„± ì‹¤íŒ¨")
            return None

        # ê²°ê³¼ ì—°ê²°
        result_list = list(results)
        if not result_list:
            logging.error("  âŒ Instruct2 ì†ë„ ì œì–´ í•©ì„± ê²°ê³¼ ì—†ìŒ")
            return None

        combined_audio = []
        for out in result_list:
            if 'tts_speech' in out:
                speech = out['tts_speech']
                if speech.device.type != 'cpu':
                    speech = speech.cpu()
                combined_audio.append(speech)

        if not combined_audio:
            logging.error("  âŒ ìœ íš¨í•œ Instruct2 ì†ë„ ì œì–´ í•©ì„± ê²°ê³¼ ì—†ìŒ")
            return None

        final_audio = torch.cat(combined_audio, dim=1)
        duration = final_audio.size(1) / 24000
        logging.info(f"  âœ… Instruct2 ì†ë„ ì œì–´ ì™„ë£Œ: {duration:.2f}ì´ˆ")

        return final_audio

    except Exception as e:
        logging.error(f"  âŒ Instruct2 ì†ë„ ì œì–´ í•©ì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None


# Gradio ì•±ì˜ postprocess í•¨ìˆ˜
def postprocess(speech: torch.Tensor,
                top_db: int = 60,
                hop_length: int = 220,
                win_length: int = 440,
                max_val: float = 0.8) -> torch.Tensor:
    """WebUIì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ prompt ì˜¤ë””ì˜¤ë§Œ ì „ì²˜ë¦¬"""
    speech, _ = librosa.effects.trim(
        speech, top_db=top_db,
        frame_length=win_length,
        hop_length=hop_length
    )
    if speech.abs().max() > max_val:
        speech = speech / speech.abs().max() * max_val
    speech = torch.concat([speech, torch.zeros(1, int(16000 * 0.2))], dim=1)  # 16kHz ê¸°ì¤€ íŒ¨ë”©
    return speech


# ì˜¤ë””ì˜¤ ë¡œë“œ ë° ë¦¬ìƒ˜í”Œ í•¨ìˆ˜
def load_wav_resample(path: str, target_sr: int = 16000, min_duration: float = 3.0) -> torch.Tensor:
    """
    ì˜¤ë””ì˜¤ ë¡œë“œ ë° ë¦¬ìƒ˜í”Œë§ (CosyVoice 3ì´ˆ ì œì•½ ìš°íšŒ)

    Args:
        path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        target_sr: ëª©í‘œ ìƒ˜í”Œë§ ë ˆì´íŠ¸
        min_duration: ìµœì†Œ ê¸¸ì´ (ì´ˆ) - CosyVoice ì œì•½ ìš°íšŒìš©
    """
    waveform, sr = torchaudio.load(path)
    if sr != target_sr:
        waveform = torchaudio.functional.resample(
            waveform, orig_freq=sr, new_freq=target_sr
        )
    # ìŠ¤í…Œë ˆì˜¤â†’ëª¨ë…¸
    if waveform.size(0) > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    # 3ì´ˆ ì œì•½ ìš°íšŒ: ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ì— ë¬´ìŒ íŒ¨ë”© ì¶”ê°€
    current_duration = waveform.size(1) / target_sr
    if current_duration < min_duration:
        needed_samples = int((min_duration - current_duration) * target_sr)
        # ìì—°ìŠ¤ëŸ¬ìš´ ë¬´ìŒ íŒ¨ë”© (ëì— ì¶”ê°€)
        padding = torch.zeros(1, needed_samples)
        waveform = torch.cat([waveform, padding], dim=1)
        logging.debug(f"Padding added: {current_duration:.2f}s â†’ {min_duration:.2f}s")

    return waveform


def optimize_prompt_audio(prompt_wav: torch.Tensor, target_sr: int = 16000) -> torch.Tensor:
    """
    í”„ë¡¬í”„íŠ¸ ì˜¤ë””ì˜¤ ìµœì í™” - ëŠ˜ì–´ì§ ë°©ì§€

    Args:
        prompt_wav: í”„ë¡¬í”„íŠ¸ ì˜¤ë””ì˜¤ í…ì„œ
        target_sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸

    Returns:
        ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì˜¤ë””ì˜¤
    """
    # ìŒì„± í™œë™ êµ¬ê°„ë§Œ ì¶”ì¶œí•˜ì—¬ ì§‘ì¤‘ë„ í–¥ìƒ
    if prompt_wav.size(1) > target_sr * 10:  # 10ì´ˆ ì´ˆê³¼ì‹œ íŠ¸ë¦¼
        # ì¤‘ê°„ ë¶€ë¶„ ì„ íƒ (ì‹œì‘/ë 1ì´ˆì”© ì œì™¸)
        start_sample = target_sr  # 1ì´ˆ
        end_sample = min(prompt_wav.size(1) - target_sr, start_sample + target_sr * 8)  # ìµœëŒ€ 8ì´ˆ
        prompt_wav = prompt_wav[:, start_sample:end_sample]
        logging.debug(f"Prompt trimmed: {prompt_wav.size(1) / target_sr:.2f}s")

    return prompt_wav


def smart_synthesis_with_length_control(cosy, text, prompt_text, prompt_wav_processed, original_duration,
                                        target_language, base_instruct_command, final_speed_ratio):
    """
    ê¸¸ì´ë¥¼ ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸ í•©ì„±: Zero-shotì´ ë„ˆë¬´ ê¸¸ë©´ Instruct2ë¡œ ì¬í•©ì„±

    Args:
        cosy: CosyVoice2 ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        text: í•©ì„±í•  í…ìŠ¤íŠ¸
        prompt_text: í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
        prompt_wav_processed: ì²˜ë¦¬ëœ í”„ë¡¬í”„íŠ¸ ì˜¤ë””ì˜¤
        original_duration: ì›ë³¸ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
        target_language: íƒ€ê²Ÿ ì–¸ì–´
        base_instruct_command: ê¸°ë³¸ instruct ëª…ë ¹ì–´
        final_speed_ratio: ì†ë„ ë¹„ìœ¨

    Returns:
        tuple: (ì„ íƒëœ ì˜¤ë””ì˜¤, ì‚¬ìš©ëœ ë°©ë²•, ì‹¤ì œ ê¸¸ì´)
    """

    # 1ë‹¨ê³„: Zero-shot í•©ì„±
    logging.info(f"  â†’ [{target_language}] Zero-shot í•©ì„± ì‹œë„...")
    results_zero = cosy.inference_zero_shot(
        text,
        prompt_text,
        prompt_wav_processed,
        "",
        stream=True,
        text_frontend=True,
        speed=final_speed_ratio
    )

    if results_zero is None:
        logging.error(f"  âŒ Zero-shot í•©ì„± ì‹¤íŒ¨")
        return None, None, 0

    # Zero-shot ê²°ê³¼ ì²˜ë¦¬
    result_list = list(results_zero)
    if not result_list:
        logging.error(f"  âŒ Zero-shot í•©ì„± ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŒ")
        return None, None, 0

    # ëª¨ë“  ê²°ê³¼ ì—°ê²°
    combined_audio = []
    for out in result_list:
        if 'tts_speech' in out:
            speech = out['tts_speech']
            if speech.device.type != 'cpu':
                speech = speech.cpu()
            combined_audio.append(speech)

    if not combined_audio:
        logging.error(f"  âŒ ìœ íš¨í•œ Zero-shot ê²°ê³¼ ì—†ìŒ")
        return None, None, 0

    zero_shot_audio = torch.cat(combined_audio, dim=1)
    zero_shot_duration = zero_shot_audio.size(1) / 24000  # 24kHz ê¸°ì¤€

    logging.info(f"  â†’ Zero-shot ê²°ê³¼: {zero_shot_duration:.2f}s (ì›ë³¸: {original_duration:.2f}s)")

    # 2ë‹¨ê³„: ê¸¸ì´ ë¹„êµ ë° íŒë‹¨
    duration_ratio = zero_shot_duration / original_duration
    tolerance = 0.1  # 50% í—ˆìš© ì˜¤ì°¨ë¡œ ì¦ê°€í•˜ì—¬ Instruct2 í˜¸ì¶œ ë¹ˆë„ ëŒ€í­ ê°ì†Œ

    if duration_ratio <= (1.0 + tolerance):
        # Zero-shot ê²°ê³¼ê°€ ì ì ˆí•¨
        logging.info(f"  âœ… Zero-shot ê¸¸ì´ ì ì ˆí•¨ (ë¹„ìœ¨: {duration_ratio:.2f})")

        # ë©”ëª¨ë¦¬ ì ˆì•½: ì¦‰ì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        import gc
        gc.collect()

        return zero_shot_audio, "zero_shot", zero_shot_duration

    # 3ë‹¨ê³„: Zero-shotì´ ë„ˆë¬´ ê¸¸ë©´ Instruct2ë¡œ ì¬ì‹œë„
    logging.info(f"  âš ï¸ Zero-shot ë„ˆë¬´ ê¹€ (ë¹„ìœ¨: {duration_ratio:.2f}) - Instruct2ë¡œ ì¬ì‹œë„")

    # ë¹ ë¥´ê²Œ ë§í•˜ê¸° ëª…ë ¹ì–´ ìƒì„±
    fast_command = get_language_specific_instruct_command("ë¹ ë¥´ê²Œ ë§í•´", target_language)
    logging.info(f"  â†’ ë¹ ë¥´ê²Œ ë§í•˜ê¸° ëª…ë ¹ì–´: '{fast_command}'")

    # Instruct2 í•©ì„± (ë” ë¹ ë¥¸ ì†ë„ë¡œ)
    faster_speed_ratio = min(final_speed_ratio * 1.2, 2.0)  # ë” ë¹ ë¥´ê²Œ ì¡°ì •
    logging.info(f"  â†’ Instruct2 ì†ë„: {faster_speed_ratio:.2f}ë°°")

    results_instruct = cosy.inference_instruct2(
        text,
        fast_command,
        prompt_wav_processed,
        stream=True,
        speed=faster_speed_ratio
    )

    if results_instruct is None:
        logging.warning(f"  âš ï¸ Instruct2 í•©ì„± ì‹¤íŒ¨ - Zero-shot ê²°ê³¼ ì‚¬ìš©")
        # ë©”ëª¨ë¦¬ ì ˆì•½
        import gc
        gc.collect()
        return zero_shot_audio, "zero_shot_fallback", zero_shot_duration

    # Instruct2 ê²°ê³¼ ì²˜ë¦¬
    instruct_result_list = list(results_instruct)
    if not instruct_result_list:
        logging.warning(f"  âš ï¸ Instruct2 ê²°ê³¼ ë¹„ì–´ìˆìŒ - Zero-shot ê²°ê³¼ ì‚¬ìš©")
        # ë©”ëª¨ë¦¬ ì ˆì•½
        import gc
        gc.collect()
        return zero_shot_audio, "zero_shot_fallback", zero_shot_duration

    # Instruct2 ê²°ê³¼ ì—°ê²°
    instruct_combined_audio = []
    for out in instruct_result_list:
        if 'tts_speech' in out:
            speech = out['tts_speech']
            if speech.device.type != 'cpu':
                speech = speech.cpu()
            instruct_combined_audio.append(speech)

    if not instruct_combined_audio:
        logging.warning(f"  âš ï¸ ìœ íš¨í•œ Instruct2 ê²°ê³¼ ì—†ìŒ - Zero-shot ê²°ê³¼ ì‚¬ìš©")
        # ë©”ëª¨ë¦¬ ì ˆì•½
        import gc
        gc.collect()
        return zero_shot_audio, "zero_shot_fallback", zero_shot_duration

    instruct_audio = torch.cat(instruct_combined_audio, dim=1)
    instruct_duration = instruct_audio.size(1) / 24000
    logging.info(f"  â†’ Instruct2 ê²°ê³¼: {instruct_duration:.2f}s")

    # 4ë‹¨ê³„: ë” ë‚˜ì€ ê²°ê³¼ ì„ íƒ
    if instruct_duration <= zero_shot_duration:
        # Instruct2ê°€ ë” ë‚˜ìŒ - Zero-shot ë©”ëª¨ë¦¬ í•´ì œ
        logging.info(f"  âœ… Instruct2 ê²°ê³¼ ì„ íƒ (ë” ì ì ˆí•œ ê¸¸ì´)")
        del zero_shot_audio  # ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ
        import gc
        gc.collect()
        return instruct_audio, "instruct2_fast", instruct_duration
    else:
        # Zero-shotì´ ì—¬ì „íˆ ë‚˜ìŒ - Instruct2 ë©”ëª¨ë¦¬ í•´ì œ
        logging.info(f"  âœ… Zero-shot ê²°ê³¼ ì„ íƒ (Instruct2ë„ ê¸¸ì–´ì§)")
        del instruct_audio  # ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ
        import gc
        gc.collect()
        return zero_shot_audio, "zero_shot_final", zero_shot_duration


# ì˜¤ë””ì˜¤ ë¶„ìœ„ê¸° ë¶„ì„ í•¨ìˆ˜ ì¶”ê°€
def analyze_audio_mood(audio_path: str) -> str:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¶„ì„í•´ì„œ ì ì ˆí•œ instruct ëª…ë ¹ì–´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # ì˜¤ë””ì˜¤ ë¡œë“œ (librosa ì‚¬ìš©)
        y, sr = librosa.load(audio_path, sr=16000)

        # 1. ìŒì„± íŠ¹ì„± ë¶„ì„
        # ìŒì„± ê°•ë„ (RMS)
        rms = librosa.feature.rms(y=y)[0]
        avg_rms = np.mean(rms)

        # ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬ (ìŒìƒ‰ ë¶„ì„)
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        avg_spectral_centroid = np.mean(spectral_centroids)

        # ì˜êµì°¨ìœ¨ (ìŒì„±ì˜ ê±°ì¹ ê¸°)
        zero_crossings = librosa.feature.zero_crossing_rate(y)[0]
        avg_zcr = np.mean(zero_crossings)

        # MFCC ê³„ìˆ˜ë¡œ ìŒì„± íŠ¹ì„± ë¶„ì„
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        mfcc_var = np.var(mfccs, axis=1).mean()  # MFCC ë³€í™”ëŸ‰

        # 2. ë¶„ìœ„ê¸° íŒë‹¨ ë¡œì§ (í…œí¬ ë¶„ì„ ì œê±°)
        if avg_rms > 0.05 and avg_spectral_centroid > 2000:
            return "í™œê¸°ì°¨ê²Œ ë§í•´"
        elif avg_rms < 0.02 and avg_spectral_centroid < 1500:
            return "ì°¨ë¶„í•˜ê²Œ ë§í•´"
        elif avg_zcr > 0.1 or avg_rms > 0.08:
            return "ê°ì •ì ìœ¼ë¡œ ë§í•´"
        elif mfcc_var < 50:
            return "ì²œì²œíˆ ë§í•´"
        elif mfcc_var > 150:
            return "ë¹ ë¥´ê²Œ ë§í•´"
        else:
            return "ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´"

    except Exception as e:
        logging.warning(f"ì˜¤ë””ì˜¤ ë¶„ìœ„ê¸° ë¶„ì„ ì‹¤íŒ¨: {e}")
        return "ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´"


def preprocess_text_for_synthesis(text: str) -> str:
    """
    í•©ì„±ìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ëŠ˜ì–´ì§ ë°©ì§€

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸

    Returns:
        ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    import re

    # ê³¼ë„í•œ ë¬¸ì¥ ë¶€í˜¸ ì œê±°
    text = re.sub(r'[.]{2,}', '.', text)  # ì—°ì† ë§ˆì¹¨í‘œ â†’ ë‹¨ì¼ ë§ˆì¹¨í‘œ
    text = re.sub(r'[,]{2,}', ',', text)  # ì—°ì† ì‰¼í‘œ â†’ ë‹¨ì¼ ì‰¼í‘œ
    text = re.sub(r'[!]{2,}', '!', text)  # ì—°ì† ëŠë‚Œí‘œ â†’ ë‹¨ì¼ ëŠë‚Œí‘œ
    text = re.sub(r'[?]{2,}', '?', text)  # ì—°ì† ë¬¼ìŒí‘œ â†’ ë‹¨ì¼ ë¬¼ìŒí‘œ

    # ê³¼ë„í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s{2,}', ' ', text)  # ì—°ì† ê³µë°± â†’ ë‹¨ì¼ ê³µë°±

    # ë¬¸ì¥ ë ì •ë¦¬ (ìì—°ìŠ¤ëŸ¬ìš´ ì¢…ë£Œë¥¼ ìœ„í•´)
    text = text.strip()
    if text and not text[-1] in '.!?':
        text += '.'  # ë¬¸ì¥ ë¶€í˜¸ê°€ ì—†ìœ¼ë©´ ë§ˆì¹¨í‘œ ì¶”ê°€

    # ë„ˆë¬´ ê¸´ ë¬¸ì¥ ë¶„í•  (80ì ì´ìƒ)
    if len(text) > 80:
        # ì‰¼í‘œë‚˜ ì ‘ì†ì‚¬ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• 
        split_points = ['í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ëŸ°ë°', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ê·¸ë˜ì„œ']
        for point in split_points:
            if point in text:
                parts = text.split(point, 1)
                if len(parts) == 2 and len(parts[0]) > 20:
                    text = parts[0].strip() + '.'
                    break

    return text


# ì–¸ì–´ë³„ íŠ¹ì„± ì •ì˜
LANGUAGE_CONFIGS = {
    'english': {
        'code': 'en',
        'name': 'English',
        'voice_style': 'natural',
        'speech_rate': 1.2,
        'phoneme_emphasis': 0.8
    },
    'chinese': {
        'code': 'zh',
        'name': 'ä¸­æ–‡',
        'voice_style': 'natural',
        'speech_rate': 1.2,
        'phoneme_emphasis': 0.8
    },
    'japanese': {
        'code': 'ja',
        'name': 'æ—¥æœ¬èª',
        'voice_style': 'natural',
        'speech_rate': 1.2,
        'phoneme_emphasis': 0.8
    },
    'korean': {
        'code': 'ko',
        'name': 'í•œêµ­ì–´',
        'voice_style': 'natural',
        'speech_rate': 1.1,
        'phoneme_emphasis': 1.0
    }
}


def detect_text_language(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    """
    import re

    # ì˜ì–´ ë¬¸ì ë¹„ìœ¨ ê³„ì‚° (ê³µë°± ì œì™¸)
    non_space_chars = [c for c in text if not c.isspace()]
    if not non_space_chars:
        return 'korean'  # ê¸°ë³¸ê°’

    english_chars = len([c for c in non_space_chars if ord(c) < 128])
    korean_chars = len([c for c in non_space_chars if 0xAC00 <= ord(c) <= 0xD7A3])
    chinese_chars = len([c for c in non_space_chars if 0x4E00 <= ord(c) <= 0x9FFF])
    japanese_chars = len([c for c in non_space_chars if (0x3040 <= ord(c) <= 0x309F)])  # íˆë¼ê°€ë‚˜ + ê°€íƒ€ì¹´ë‚˜

    total_chars = len(non_space_chars)

    # ë¹„ìœ¨ ê³„ì‚°
    english_ratio = english_chars / total_chars
    korean_ratio = korean_chars / total_chars
    chinese_ratio = chinese_chars / total_chars
    japanese_ratio = japanese_chars / total_chars

    logging.debug(
        f"Language detection ratios - EN: {english_ratio:.2f}, KO: {korean_ratio:.2f}, ZH: {chinese_ratio:.2f}, JA: {japanese_ratio:.2f}")

    # ì–¸ì–´ íŒë³„ (ì„ê³„ê°’ ê¸°ë°˜)
    if english_ratio > 0.7:
        detected_lang = 'english'
    elif korean_ratio > 0.5:
        detected_lang = 'korean'
    elif chinese_ratio > 0.5:
        detected_lang = 'chinese'
    elif japanese_ratio > 0.3:
        detected_lang = 'japanese'
    else:
        # í˜¼ì¬ëœ ê²½ìš°, ê°€ì¥ ë†’ì€ ë¹„ìœ¨ ì„ íƒ
        ratios = {
            'english': english_ratio,
            'korean': korean_ratio,
            'chinese': chinese_ratio,
            'japanese': japanese_ratio
        }
        detected_lang = max(ratios, key=ratios.get)

    logging.info(f"Detected language: {detected_lang} for text: '{text[:50]}{'...' if len(text) > 50 else ''}'")
    return detected_lang


def preprocess_text_by_language(text: str, target_language: str) -> str:
    """
    ì–¸ì–´ë³„ íŠ¹ì„±ì— ë§ëŠ” í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    """
    import re

    # ê¸°ë³¸ ì •ë¦¬
    text = text.strip()

    if target_language == 'english':
        # ì˜ì–´: ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ êµ¬ì¡° ìœ ì§€
        text = re.sub(r'\s+', ' ', text)  # ì—°ì† ê³µë°± ì •ë¦¬
        text = re.sub(r'[.]{2,}', '.', text)  # ì—°ì† ë§ˆì¹¨í‘œ ì •ë¦¬

        # ì˜ì–´ íŠ¹ìˆ˜ ì²˜ë¦¬: ì•½ì–´ ì²˜ë¦¬
        text = re.sub(r'\bDr\.\s*', 'Doctor ', text)
        text = re.sub(r'\bMr\.\s*', 'Mister ', text)
        text = re.sub(r'\bMrs\.\s*', 'Misses ', text)

    elif target_language == 'chinese':
        # ì¤‘êµ­ì–´: ê°„ì²´ì ìš°ì„ , ì„±ì¡° ê³ ë ¤
        text = re.sub(r'[ï¼Œ]{2,}', 'ï¼Œ', text)  # ì—°ì† ì‰¼í‘œ ì •ë¦¬
        text = re.sub(r'[ã€‚]{2,}', 'ã€‚', text)  # ì—°ì† ë§ˆì¹¨í‘œ ì •ë¦¬

    elif target_language == 'japanese':
        # ì¼ë³¸ì–´: ë†’ì„ë§ ì²˜ë¦¬, ìì—°ìŠ¤ëŸ¬ìš´ ì¢…ê²°ì–´ë¯¸
        text = re.sub(r'[ã€]{2,}', 'ã€', text)  # ì—°ì† ë…ì  ì •ë¦¬
        text = re.sub(r'[ã€‚]{2,}', 'ã€‚', text)  # ì—°ì† ë§ˆì¹¨í‘œ ì •ë¦¬

    elif target_language == 'korean':
        # í•œêµ­ì–´: ì¡´ëŒ“ë§ ì²˜ë¦¬
        text = re.sub(r'[,]{2,}', ',', text)  # ì—°ì† ì‰¼í‘œ ì •ë¦¬
        text = re.sub(r'[.]{2,}', '.', text)  # ì—°ì† ë§ˆì¹¨í‘œ ì •ë¦¬

    # ê³µí†µ ì²˜ë¦¬: ê³¼ë„í•œ ê°íƒ„ì‚¬ ì œê±°
    text = re.sub(r'[!]{2,}', '!', text)
    text = re.sub(r'[?]{2,}', '?', text)

    return text


def get_language_specific_instruct_command(base_command: str, target_language: str) -> str:
    """
    ì–¸ì–´ë³„ íŠ¹ì„±ì— ë§ëŠ” instruct ëª…ë ¹ì–´ ìƒì„± (CosyVoice2 ë…¼ë¬¸ ê¸°ë°˜ í™•ì¥)
    """
    # CosyVoice2ì—ì„œ ì§€ì›í•˜ëŠ” ë” ë‹¤ì–‘í•œ ì†ë„ ë° ìŠ¤íƒ€ì¼ ì œì–´ ëª…ë ¹ì–´
    if target_language == 'english':
        return {
            # ì†ë„ ì œì–´
            'ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´': 'Speak naturally with normal pace',
            'í™œê¸°ì°¨ê²Œ ë§í•´': 'Speak with energy and enthusiasm',
            'ì°¨ë¶„í•˜ê²Œ ë§í•´': 'Speak calmly and peacefully',
            'ê°ì •ì ìœ¼ë¡œ ë§í•´': 'Speak with strong emotion',
            'ì²œì²œíˆ ë§í•´': 'Speak slowly and clearly',
            'ë¹ ë¥´ê²Œ ë§í•´': 'Speak quickly and briskly',
            'ë§¤ìš° ì²œì²œíˆ ë§í•´': 'Speak very slowly with clear articulation',
            'ë§¤ìš° ë¹ ë¥´ê²Œ ë§í•´': 'Speak very quickly but distinctly',
            # ì¶”ê°€ ìŠ¤íƒ€ì¼ ì œì–´ (CosyVoice2 ë…¼ë¬¸ ê¸°ë°˜)
            'ë¶€ë“œëŸ½ê²Œ ë§í•´': 'Speak softly and gently',
            'í˜ì°¨ê²Œ ë§í•´': 'Speak with strong voice and power',
            'ì†ì‚­ì´ë“¯ ë§í•´': 'Speak in a whisper',
            'ë˜ë ·í•˜ê²Œ ë§í•´': 'Speak with clear pronunciation',
            'ë¦¬ë“¬ê° ìˆê²Œ ë§í•´': 'Speak with good rhythm and flow'
        }.get(base_command, base_command)

    elif target_language == 'chinese':
        return {
            # ì†ë„ ì œì–´
            'ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´': 'è‡ªç„¶åœ°è¯´è¯',
            'í™œê¸°ì°¨ê²Œ ë§í•´': 'å……æ»¡æ´»åŠ›åœ°è¯´è¯',
            'ì°¨ë¶„í•˜ê²Œ ë§í•´': 'å¹³é™åœ°è¯´è¯',
            'ê°ì •ì ìœ¼ë¡œ ë§í•´': 'å¯Œæœ‰æ„Ÿæƒ…åœ°è¯´è¯',
            'ì²œì²œíˆ ë§í•´': 'æ…¢æ…¢åœ°è¯´è¯',
            'ë¹ ë¥´ê²Œ ë§í•´': 'å¿«é€Ÿåœ°è¯´è¯',
            'ë§¤ìš° ì²œì²œíˆ ë§í•´': 'éå¸¸æ…¢åœ°è¯´è¯',
            'ë§¤ìš° ë¹ ë¥´ê²Œ ë§í•´': 'éå¸¸å¿«åœ°è¯´è¯',
            # ì¶”ê°€ ìŠ¤íƒ€ì¼ ì œì–´
            'ë¶€ë“œëŸ½ê²Œ ë§í•´': 'æ¸©æŸ”åœ°è¯´è¯',
            'í˜ì°¨ê²Œ ë§í•´': 'æœ‰åŠ›åœ°è¯´è¯',
            'ì†ì‚­ì´ë“¯ ë§í•´': 'è½»å£°è¯´è¯',
            'ë˜ë ·í•˜ê²Œ ë§í•´': 'æ¸…æ¥šåœ°è¯´è¯',
            'ë¦¬ë“¬ê° ìˆê²Œ ë§í•´': 'æœ‰èŠ‚å¥åœ°è¯´è¯'
        }.get(base_command, base_command)

    elif target_language == 'japanese':
        return {
            # ì†ë„ ì œì–´
            'ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´': 'è‡ªç„¶ã«è©±ã—ã¦ãã ã•ã„',
            'í™œê¸°ì°¨ê²Œ ë§í•´': 'å…ƒæ°—ã‚ˆãè©±ã—ã¦ãã ã•ã„',
            'ì°¨ë¶„í•˜ê²Œ ë§í•´': 'è½ã¡ç€ã„ã¦è©±ã—ã¦ãã ã•ã„',
            'ê°ì •ì ìœ¼ë¡œ ë§í•´': 'æ„Ÿæƒ…è±Šã‹ã«è©±ã—ã¦ãã ã•ã„',
            'ì²œì²œíˆ ë§í•´': 'ã‚†ã£ãã‚Šã¨è©±ã—ã¦ãã ã•ã„',
            'ë¹ ë¥´ê²Œ ë§í•´': 'é€Ÿãè©±ã—ã¦ãã ã•ã„',
            'ë§¤ìš° ì²œì²œíˆ ë§í•´': 'éå¸¸ã«ã‚†ã£ãã‚Šè©±ã—ã¦ãã ã•ã„',
            'ë§¤ìš° ë¹ ë¥´ê²Œ ë§í•´': 'éå¸¸ã«é€Ÿãè©±ã—ã¦ãã ã•ã„',
            # ì¶”ê°€ ìŠ¤íƒ€ì¼ ì œì–´
            'ë¶€ë“œëŸ½ê²Œ ë§í•´': 'å„ªã—ãè©±ã—ã¦ãã ã•ã„',
            'í˜ì°¨ê²Œ ë§í•´': 'åŠ›å¼·ãè©±ã—ã¦ãã ã•ã„',
            'ì†ì‚­ì´ë“¯ ë§í•´': 'å›ãã‚ˆã†ã«è©±ã—ã¦ãã ã•ã„',
            'ë˜ë ·í•˜ê²Œ ë§í•´': 'ã¯ã£ãã‚Šã¨è©±ã—ã¦ãã ã•ã„',
            'ë¦¬ë“¬ê° ìˆê²Œ ë§í•´': 'ãƒªã‚ºãƒ ã‚ˆãè©±ã—ã¦ãã ã•ã„'
        }.get(base_command, base_command)

    else:  # Korean (default)
        return {
            # ì†ë„ ì œì–´
            'ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´': 'ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´',
            'í™œê¸°ì°¨ê²Œ ë§í•´': 'í™œê¸°ì°¨ê²Œ ë§í•´',
            'ì°¨ë¶„í•˜ê²Œ ë§í•´': 'ì°¨ë¶„í•˜ê²Œ ë§í•´',
            'ê°ì •ì ìœ¼ë¡œ ë§í•´': 'ê°ì •ì ìœ¼ë¡œ ë§í•´',
            'ì²œì²œíˆ ë§í•´': 'ì²œì²œíˆ ë§í•´',
            'ë¹ ë¥´ê²Œ ë§í•´': 'ë¹ ë¥´ê²Œ ë§í•´',
            'ë§¤ìš° ì²œì²œíˆ ë§í•´': 'ë§¤ìš° ì²œì²œíˆ ë§í•´',
            'ë§¤ìš° ë¹ ë¥´ê²Œ ë§í•´': 'ë§¤ìš° ë¹ ë¥´ê²Œ ë§í•´',
            # ì¶”ê°€ ìŠ¤íƒ€ì¼ ì œì–´ (CosyVoice2 ë…¼ë¬¸ ê¸°ë°˜)
            'ë¶€ë“œëŸ½ê²Œ ë§í•´': 'ë¶€ë“œëŸ½ê²Œ ë§í•´',
            'í˜ì°¨ê²Œ ë§í•´': 'í˜ì°¨ê²Œ ë§í•´',
            'ì†ì‚­ì´ë“¯ ë§í•´': 'ì†ì‚­ì´ë“¯ ë§í•´',
            'ë˜ë ·í•˜ê²Œ ë§í•´': 'ë˜ë ·í•˜ê²Œ ë§í•´',
            'ë¦¬ë“¬ê° ìˆê²Œ ë§í•´': 'ë¦¬ë“¬ê° ìˆê²Œ ë§í•´'
        }.get(base_command, base_command)


# ë°°ì¹˜ í•©ì„± í•¨ìˆ˜
def main(audio_dir, prompt_text_dir, text_dir, out_dir, model_path=LOCAL_COSYVOICE_MODEL, enable_instruct=True,
         manual_command=None, target_language=None):
    # Device ì„¤ì • (MPS ì§€ì› ì œì™¸)
    if torch.cuda.is_available():
        device = torch.device("cuda")
        logging.info("CUDA GPU ê°€ì† ì‚¬ìš© ì¤‘")
    else:
        device = torch.device("cpu")
        logging.info("CPU ì‚¬ìš© ì¤‘")

    # ì…ë ¥ ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    missing_dirs = []
    if not os.path.exists(audio_dir):
        missing_dirs.append(f"ì˜¤ë””ì˜¤ ë””ë ‰í† ë¦¬: {audio_dir}")
    if not os.path.exists(prompt_text_dir):
        missing_dirs.append(f"í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬: {prompt_text_dir}")
    if not os.path.exists(text_dir):
        missing_dirs.append(f"ëŒ€ìƒ í…ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬: {text_dir}")

    if missing_dirs:
        logging.error("í•„ìš”í•œ ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:")
        for missing_dir in missing_dirs:
            logging.error(f"   - {missing_dir}")
        logging.error("âš ï¸ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # íƒ€ê²Ÿ ì–¸ì–´ ìë™ ê°ì§€ (ê²½ë¡œì—ì„œ ì¶”ì¶œ)
    if target_language is None:
        # text_dir ê²½ë¡œì—ì„œ ì–¸ì–´ ì¶”ì¶œ (ì˜ˆ: .../english/free/)
        path_parts = text_dir.split(os.sep)
        for part in path_parts:
            if part.lower() in LANGUAGE_CONFIGS:
                target_language = part.lower()
                break

        if target_language is None:
            target_language = 'korean'  # ê¸°ë³¸ê°’

    logging.info(f"Target language detected/set: {target_language}")

    # ëª¨ë¸ ì´ˆê¸°í™”
    global cosy
    cosy = CosyVoice2(
        model_path,
        load_jit=False,
        load_trt=False,
        load_vllm=False,
        fp16=False
    )

    # ì…ë ¥ íŒŒì¼ ëª©ë¡
    try:
        audio_files = sorted([f for f in os.listdir(audio_dir) if f.lower().endswith('.wav')])
        prompt_files = sorted([f for f in os.listdir(prompt_text_dir) if f.lower().endswith('.txt')])
        text_files = sorted([f for f in os.listdir(text_dir) if f.lower().endswith('.txt')])
    except Exception as e:
        logging.error(f"ì…ë ¥ íŒŒì¼ ëª©ë¡ì„ ì½ëŠ” ë° ì‹¤íŒ¨: {e}")
        return

    logging.info(
        f"[ë””ë²„ê·¸] ì˜¤ë””ì˜¤ íŒŒì¼ ({len(audio_files)}): {audio_files[:5]}{'...' if len(audio_files) > 5 else ''}")
    logging.info(
        f"[ë””ë²„ê·¸] í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ íŒŒì¼ ({len(prompt_files)}): {prompt_files[:5]}{'...' if len(prompt_files) > 5 else ''}")
    logging.info(
        f"[ë””ë²„ê·¸] ëŒ€ìƒ í…ìŠ¤íŠ¸ íŒŒì¼ ({len(text_files)}): {text_files[:5]}{'...' if len(text_files) > 5 else ''}")

    # íŒŒì¼ ë§¤ì¹­ ê°œì„ : ì‹¤ì œ íŒŒì¼ëª… íŒ¨í„´ì— ë§ê²Œ ë§¤ì¹­
    matched_files = []

    for i, audio_file in enumerate(audio_files):
        # ì˜¤ë””ì˜¤ íŒŒì¼ëª…ì—ì„œ ê¸°ë³¸ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: vocal_video22_extracted.wav_10_001.wav -> vocal_video22_extracted.wav_10_001)
        audio_base = os.path.splitext(audio_file)[0]

        # ì•ˆì „í•œ íŒŒì¼ëª… ë¶„í•  ì²˜ë¦¬
        try:
            if '_' in audio_base:
                # "ì¡°ìš©ì„_1m_001"ì—ì„œ "ì¡°ìš©ì„_1m"ì™€ "001" ë¶„ë¦¬
                parts = audio_base.rsplit('_', 1)
                if len(parts) >= 2 and parts[-1].isdigit():
                    audio_base = '_'.join(parts[:-1])  # ë§ˆì§€ë§‰ ìˆ«ì ë¶€ë¶„ ì œì™¸í•œ ëª¨ë“  ë¶€ë¶„
                    segment_num = parts[-1]  # ë§ˆì§€ë§‰ ìˆ«ì ë¶€ë¶„
                else:
                    segment_num = f"{i:03d}"
            else:
                segment_num = f"{i:03d}"
        except Exception as e:
            logging.warning(f"íŒŒì¼ëª… ì²˜ë¦¬ ì˜¤ë¥˜ ({audio_file}): {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            segment_num = f"{i:03d}"

        # í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸° (ì˜ˆ: vocal_video22_extracted.wav_10_001.ko.txt)
        prompt_file = f"{audio_base}_{segment_num}.ko.txt"
        prompt_file_path = os.path.join(prompt_text_dir, prompt_file)

        # ëŒ€ìƒ í…ìŠ¤íŠ¸ íŒŒì¼ ì°¾ê¸° (ì˜ˆ: vocal_video22_extracted.wav_10_001.ko.txt)
        target_file = f"{audio_base}_{segment_num}.ko.txt"
        target_file_path = os.path.join(text_dir, target_file)

        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if os.path.exists(prompt_file_path) and os.path.exists(target_file_path):
            matched_files.append((audio_file, prompt_file, target_file))
        else:
            missing_files = []
            if not os.path.exists(prompt_file_path):
                missing_files.append(f"í”„ë¡¬í”„íŠ¸({prompt_file})")
            if not os.path.exists(target_file_path):
                missing_files.append(f"ëŒ€ìƒ({target_file})")
            logging.warning(f"[ê±´ë„ˆëœ€] {audio_file} - ëˆ„ë½ëœ íŒŒì¼: {', '.join(missing_files)}")

    logging.info(f"[ì •ë³´] ë§¤ì¹­ëœ íŒŒì¼ ì„¸íŠ¸: {len(matched_files)} / {len(audio_files)} ê°œ")

    if len(matched_files) == 0:
        logging.error("âŒ ë§¤ì¹­ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ëª… íŒ¨í„´ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ê°•ì œ ìƒì„±
    zero_shot_dir = os.path.join(out_dir, 'zero_shot')
    instruct_dir = os.path.join(out_dir, 'instruct')

    try:
        os.makedirs(zero_shot_dir, exist_ok=True)
        os.makedirs(instruct_dir, exist_ok=True)
        logging.info(f"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ:")
        logging.info(f"   - Zero-shot: {zero_shot_dir}")
        logging.info(f"   - Instruct: {instruct_dir}")
    except Exception as e:
        logging.error(f"âŒ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        return

    current_seed = random.randint(0, 2**32 - 1)
    random.seed(current_seed)
    np.random.seed(current_seed)
    torch.manual_seed(current_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(current_seed)
    logging.info(f"ëœë¤ ì‹œë“œ ì‚¬ìš© ì¤‘: {current_seed} / ë””ë°”ì´ìŠ¤: {device}")

    # íŒŒì¼ë³„ í•©ì„±
    for i, (awav, ptxt, txt) in enumerate(matched_files, 1):
        # íŒŒì¼ ê²½ë¡œ ì•ˆì „í™”
        safe_awav = sanitize_filename(awav)
        safe_ptxt = sanitize_filename(ptxt)
        safe_txt = sanitize_filename(txt)

        wav_path = safe_file_operations(os.path.join(audio_dir, awav), "read")
        ptxt_path = safe_file_operations(os.path.join(prompt_text_dir, ptxt), "read")
        txt_path = safe_file_operations(os.path.join(text_dir, txt), "read")

        # íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜ ì²´í¬
        if wav_path.startswith("âŒ") or ptxt_path.startswith("âŒ") or txt_path.startswith("âŒ"):
            logging.error(f"  âŒ íŒŒì¼ ê²½ë¡œ ì˜¤ë¥˜:")
            if wav_path.startswith("âŒ"):
                logging.error(f"    - {wav_path}")
            if ptxt_path.startswith("âŒ"):
                logging.error(f"    - {ptxt_path}")
            if txt_path.startswith("âŒ"):
                logging.error(f"    - {txt_path}")
            continue

        # ë¡œê¹… (ì•ˆì „í™”ëœ íŒŒì¼ëª… í‘œì‹œ)
        if safe_awav != awav or safe_ptxt != ptxt or safe_txt != txt:
            logging.info(f"[{i}/{len(matched_files)}] ì²˜ë¦¬ ì¤‘ (íŒŒì¼ëª… ì•ˆì „í™”ë¨)")
            logging.info(f"  â†’ ì˜¤ë””ì˜¤: {awav} â†’ {safe_awav}")
            logging.info(f"  â†’ í”„ë¡¬í”„íŠ¸: {ptxt} â†’ {safe_ptxt}")
            logging.info(f"  â†’ í…ìŠ¤íŠ¸: {txt} â†’ {safe_txt}")
        else:
            logging.info(f"[{i}/{len(matched_files)}] ì²˜ë¦¬ ì¤‘ â†’ {awav} / {ptxt} / {txt}")

        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ë¡œê¹…
        missing_files = []
        if not os.path.exists(wav_path):
            missing_files.append(f"ì˜¤ë””ì˜¤: {wav_path}")
        if not os.path.exists(ptxt_path):
            missing_files.append(f"í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸: {ptxt_path}")
        if not os.path.exists(txt_path):
            missing_files.append(f"ëŒ€ìƒ í…ìŠ¤íŠ¸: {txt_path}")

        if missing_files:
            logging.error(f"  âŒ ëˆ„ë½ëœ íŒŒì¼: {', '.join(missing_files)}")
            continue

        # ì˜¤ë””ì˜¤ & í…ìŠ¤íŠ¸ ë¡œë“œ
        try:
            prompt_wav = load_wav_resample(wav_path)
        except Exception as e:
            logging.error(f"  âŒ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ ({wav_path}): {e}")
            continue

        # ì›ë³¸ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¶”ì  (íŒ¨ë”© ì—†ì´ ì •í™•í•œ ê¸¸ì´)
        try:
            original_wav = load_wav_resample(wav_path, min_duration=0.0)  # íŒ¨ë”© ì—†ì´ ë¡œë“œ
            original_duration = original_wav.size(1) / 16000  # ì´ˆ ë‹¨ìœ„
        except Exception as e:
            logging.error(f"  âŒ ì›ë³¸ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì‹¤íŒ¨: {e}")
            continue

        try:
            with open(ptxt_path, 'r', encoding='utf-8') as f:
                prompt_text = f.read().strip()
            with open(txt_path, 'r', encoding='utf-8') as f:
                text = f.read().strip()
        except Exception as e:
            logging.error(f"  âŒ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            continue

        # í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì‚¬
        if not prompt_text or len(prompt_text.strip()) == 0:
            logging.error(f"  âŒ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤: {ptxt_path}")
            continue
        if not text or len(text.strip()) == 0:
            logging.error(f"  âŒ ëŒ€ìƒ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤: {txt_path}")
            continue

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¶”ê°€ (ëŠ˜ì–´ì§ ë°©ì§€)
        original_text = text
        original_prompt_text = prompt_text
        text = preprocess_text_for_synthesis(text)
        prompt_text = preprocess_text_for_synthesis(prompt_text)

        # ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œê¹…
        if text != original_text:
            logging.info(f"  â†’ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: '{original_text[:30]}...' â†’ '{text[:30]}...'")
        if prompt_text != original_prompt_text:
            logging.info(f"  â†’ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: '{original_prompt_text[:20]}...' â†’ '{prompt_text[:20]}...'")

        # íŒŒì¼ëª…ì—ì„œ ê¸°ë³¸ ì´ë¦„ê³¼ ì„¸ê·¸ë¨¼íŠ¸ ë²ˆí˜¸ ì¶”ì¶œ
        base_name = os.path.splitext(awav)[0]  # ì˜ˆ: "ì¡°ìš©ì„_1m_001"
        if '_' in base_name:
            # "ì¡°ìš©ì„_1m_001"ì—ì„œ "ì¡°ìš©ì„_1m"ì™€ "001" ë¶„ë¦¬
            parts = base_name.rsplit('_', 1)
            if len(parts) == 2 and parts[1].isdigit():
                audio_base = parts[0]  # "ì¡°ìš©ì„_1m"
                segment_num = parts[1]  # "001"
            else:
                audio_base = base_name
                segment_num = f"{i:03d}"
        else:
            audio_base = base_name
            segment_num = f"{i:03d}"

        logging.info(f"  â†’ [{target_language}] í”„ë¡¬í”„íŠ¸ ì˜¤ë””ì˜¤ ê¸¸ì´: {prompt_wav.size(1) / 16000:.2f}s")

        # í•©ì„± ì „ í•„ìˆ˜ ì¡°ê±´ ì¬í™•ì¸
        if prompt_wav is None or prompt_wav.size(1) == 0:
            logging.error(f"  âŒ ì²˜ë¦¬ëœ í”„ë¡¬í”„íŠ¸ ì˜¤ë””ì˜¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤")
            continue
        if not text.strip() or not prompt_text.strip():
            logging.error(f"  âŒ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤")
            continue

        # ì–¸ì–´ë³„ ì†ë„ ì¡°ì • ì ìš©
        lang_config = LANGUAGE_CONFIGS.get(target_language, LANGUAGE_CONFIGS['korean'])
        base_speed_ratio = lang_config['speech_rate']

        # Zero-shot ë° Instruct2 í•©ì„±
        audio_result, method_used, duration = smart_synthesis_with_length_control(
            cosy,
            text,
            prompt_text,
            prompt_wav,
            original_duration,
            target_language,
            manual_command if manual_command else 'ìì—°ìŠ¤ëŸ½ê²Œ ë§í•´',
            base_speed_ratio
        )

        if audio_result is not None:
            logging.info(f"  âœ… [{target_language}] ìµœì¢… ê²°ê³¼: {duration:.2f}s (ë°©ë²•: {method_used})")
        else:
            logging.error(f"  âŒ [{target_language}] í•©ì„± ê²°ê³¼ ì—†ìŒ")
            continue

        # ê²°ê³¼ ì €ì¥
        if method_used == "instruct2_fast":
            try:
                logging.info(f"  â†’ [{target_language}] Instruct2 ê²°ê³¼ ì €ì¥ ì‹œì‘...")

                # Instruct2 ë””ë ‰í† ë¦¬ í™•ì‹¤íˆ ìƒì„±
                if not os.path.exists(instruct_dir):
                    os.makedirs(instruct_dir, exist_ok=True)
                    logging.info(f"  â†’ Instruct2 ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {instruct_dir}")

                # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
                safe_name = sanitize_filename(f"{audio_base}_{segment_num}_instruct.wav")
                save_path = os.path.join(instruct_dir, safe_name)

                try:
                    torchaudio.save(save_path, audio_result, 24000)
                    final_duration = audio_result.size(1) / 24000

                    # íŒŒì¼ ì €ì¥ í™•ì¸
                    if os.path.exists(save_path):
                        file_size = os.path.getsize(save_path)
                        logging.info(
                            f"    âœ… Instruct2 ì €ì¥ ì™„ë£Œ âœ {safe_name} ({final_duration:.2f}ì´ˆ, {file_size} ë°”ì´íŠ¸)")
                    else:
                        logging.error(f"    âŒ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {save_path}")

                except Exception as save_error:
                    logging.error(f"    âŒ Instruct2 íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {save_error}")
            except Exception as e:
                logging.error(f"    âŒ [{target_language}] Instruct2 ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                import traceback
                logging.error(f"    ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        else:
            # Zero-shot ê²°ê³¼ ì €ì¥
            try:
                logging.info(f"  â†’ [{target_language}] Zero-shot ê²°ê³¼ ì €ì¥ ì‹œì‘...")

                # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
                safe_name = sanitize_filename(f"{audio_base}_{segment_num}.wav")
                save_path = os.path.join(zero_shot_dir, safe_name)

                # ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
                if not os.path.exists(zero_shot_dir):
                    os.makedirs(zero_shot_dir, exist_ok=True)

                try:
                    torchaudio.save(save_path, audio_result, 24000)
                    final_duration = audio_result.size(1) / 24000

                    # íŒŒì¼ ì €ì¥ í™•ì¸
                    if os.path.exists(save_path):
                        file_size = os.path.getsize(save_path)
                        logging.info(
                            f"    âœ… Zero-shot ì €ì¥ ì™„ë£Œ âœ {safe_name} ({final_duration:.2f}ì´ˆ, {file_size} ë°”ì´íŠ¸)")
                    else:
                        logging.error(f"    âŒ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {save_path}")

                except Exception as save_error:
                    logging.error(f"    âŒ torchaudio.save ì‹¤íŒ¨: {save_error}")
            except Exception as e:
                logging.error(f"  âŒ [{target_language}] Zero-shot ì €ì¥ ì‹¤íŒ¨: {e}")
                import traceback
                logging.error(f"  ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ê° íŒŒì¼ ì²˜ë¦¬ í›„)
        cleanup_memory(device)

    # CosyVoice ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
    cleanup_cosyvoice_model()

    logging.info(f"âœ… [{target_language}] ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ - {len(matched_files)} ê°œ íŒŒì¼ ì²˜ë¦¬ë¨")


# ìŠ¤í¬ë¦½íŠ¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="ë°°ì¹˜ Zero-shot TTS with CosyVoice2 (Language-aware)"
    )
    parser.add_argument('--audio_dir', required=True, help="ì°¸ì¡° .wav í´ë” ê²½ë¡œ")
    parser.add_argument('--prompt_text_dir', required=True, help="í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ (.txt) í´ë” ê²½ë¡œ")
    parser.add_argument('--text_dir', required=True, help="í•©ì„± í…ìŠ¤íŠ¸ (.txt) í´ë” ê²½ë¡œ")
    parser.add_argument('--out_dir', required=True, help="ì¶œë ¥ WAV í´ë” ê²½ë¡œ")
    parser.add_argument('--model_path', default=LOCAL_COSYVOICE_MODEL, help="CosyVoice2 ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument('--enable_instruct', action='store_true', default=False, help="Instruct2 ê¸°ëŠ¥ í™œì„±í™”")
    parser.add_argument('--manual_command', type=str, default=None, help="ìˆ˜ë™ ì§€ì • instruct ëª…ë ¹ì–´")
    parser.add_argument('--target_language', type=str, default=None, help="íƒ€ê²Ÿ ì–¸ì–´ (english/chinese/japanese/korean)")
    args = parser.parse_args()

    main(
        args.audio_dir,
        args.prompt_text_dir,
        args.text_dir,
        args.out_dir,
        model_path=args.model_path,
        enable_instruct=args.enable_instruct,
        manual_command=args.manual_command,
        target_language=args.target_language
    )
