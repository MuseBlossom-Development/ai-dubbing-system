import os
import numpy as np
from utils import log_message


def analyze_speakers_simple(segments, audio_path):
    """
    ê°„ë‹¨í•œ í™”ì ë¶„ì„ (librosa ê¸°ë°˜)
    
    Args:
        segments: [(start_ms, end_ms), ...] ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        speaker_labels: [0, 0, 1, 1, 0, ...] í™”ì ë ˆì´ë¸” (ê°™ì€ ìˆ«ì = ê°™ì€ í™”ì)
    """
    try:
        import librosa
        from sklearn.cluster import AgglomerativeClustering
        from sklearn.metrics.pairwise import cosine_similarity

        log_message("ğŸ¤ í™”ì ë¶„ì„ ì‹œì‘...")

        embeddings = []
        valid_segments = []

        # ê° ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ìŒì„± íŠ¹ì§• ì¶”ì¶œ
        for i, (start_ms, end_ms) in enumerate(segments):
            try:
                # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if end_ms - start_ms < 500:  # 0.5ì´ˆ ë¯¸ë§Œ
                    embeddings.append(None)
                    continue

                # ì˜¤ë””ì˜¤ ë¡œë“œ
                audio, sr = librosa.load(
                    audio_path,
                    offset=start_ms / 1000,
                    duration=(end_ms - start_ms) / 1000,
                    sr=16000  # 16kHzë¡œ í†µì¼
                )

                if len(audio) < 1000:  # ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤
                    embeddings.append(None)
                    continue

                # í™”ì íŠ¹ì§• ì¶”ì¶œ (MFCC + Spectral features)
                mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
                spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)
                spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)
                zero_crossing_rate = librosa.feature.zero_crossing_rate(audio)

                # íŠ¹ì§•ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
                features = np.concatenate([
                    np.mean(mfcc, axis=1),
                    np.mean(spectral_centroid),
                    np.mean(spectral_rolloff),
                    np.mean(zero_crossing_rate)
                ])

                embeddings.append(features)
                valid_segments.append(i)

            except Exception as e:
                log_message(f"ì„¸ê·¸ë¨¼íŠ¸ {i} ë¶„ì„ ì‹¤íŒ¨: {e}")
                embeddings.append(None)

        # ìœ íš¨í•œ ì„ë² ë”©ë§Œ í•„í„°ë§
        valid_embeddings = [emb for emb in embeddings if emb is not None]

        if len(valid_embeddings) < 2:
            log_message("âš ï¸ í™”ì ë¶„ì„ì„ ìœ„í•œ ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê°™ì€ í™”ìë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            return [0] * len(segments)

        # í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ í™”ì êµ¬ë¶„
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê³„ì¸µ í´ëŸ¬ìŠ¤í„°ë§
        similarity_matrix = cosine_similarity(valid_embeddings)
        distance_matrix = 1 - similarity_matrix

        # ìë™ìœ¼ë¡œ í™”ì ìˆ˜ ê²°ì • (ê±°ë¦¬ ì„ê³„ê°’ ê¸°ë°˜)
        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=0.3,  # í™”ì êµ¬ë¶„ ë¯¼ê°ë„ (ë‚®ì„ìˆ˜ë¡ ë” ë§ì€ í™”ì)
            metric='precomputed',
            linkage='average'
        )

        cluster_labels = clustering.fit_predict(distance_matrix)

        # ê²°ê³¼ë¥¼ ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸ ìˆœì„œì— ë§ì¶° ì •ë¦¬
        speaker_labels = []
        valid_idx = 0

        for i, embedding in enumerate(embeddings):
            if embedding is not None:
                speaker_labels.append(cluster_labels[valid_idx])
                valid_idx += 1
            else:
                # ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì´ì „ í™”ìì™€ ê°™ë‹¤ê³  ê°€ì •
                if speaker_labels:
                    speaker_labels.append(speaker_labels[-1])
                else:
                    speaker_labels.append(0)

        num_speakers = len(set(speaker_labels))
        log_message(f"âœ… í™”ì ë¶„ì„ ì™„ë£Œ: {num_speakers}ëª…ì˜ í™”ì ê°ì§€")

        # í™”ìë³„ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ì¶œë ¥
        for speaker_id in set(speaker_labels):
            count = speaker_labels.count(speaker_id)
            log_message(f"   í™”ì {speaker_id}: {count}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

        return speaker_labels

    except ImportError:
        log_message("âŒ librosa ë˜ëŠ” sklearnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        log_message("pip install librosa scikit-learn ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return [0] * len(segments)  # ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê°™ì€ í™”ìë¡œ ì²˜ë¦¬

    except Exception as e:
        log_message(f"âŒ í™”ì ë¶„ì„ ì˜¤ë¥˜: {e}")
        return [0] * len(segments)  # ì—ëŸ¬ ì‹œ ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê°™ì€ í™”ìë¡œ ì²˜ë¦¬


def analyze_speakers_pyannote(audio_path):
    """
    pyannote.audioë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ í™”ì ë¶„ì„
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        speaker_timeline: [{'start': 0, 'end': 3200, 'speaker': 'SPEAKER_00'}, ...]
    """
    try:
        from pyannote.audio import Pipeline
        import torch

        log_message("ğŸ¤ pyannote.audioë¡œ í™”ì ë¶„ì„ ì‹œì‘...")

        # í™”ì ë¶„ë¦¬ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token="YOUR_HF_TOKEN"  # Hugging Face í† í° í•„ìš”
        )

        # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ
        if torch.cuda.is_available():
            pipeline = pipeline.to(torch.device("cuda"))

        # í™”ì ë¶„ë¦¬ ì‹¤í–‰
        diarization = pipeline(audio_path)

        # ê²°ê³¼ ì •ë¦¬
        speaker_timeline = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            speaker_timeline.append({
                'start': turn.start * 1000,  # msë¡œ ë³€í™˜
                'end': turn.end * 1000,
                'speaker': speaker
            })

        num_speakers = len(set([s['speaker'] for s in speaker_timeline]))
        log_message(f"âœ… pyannote í™”ì ë¶„ì„ ì™„ë£Œ: {num_speakers}ëª…ì˜ í™”ì ê°ì§€")

        return speaker_timeline

    except ImportError:
        log_message("âŒ pyannote.audioê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        log_message("pip install pyannote.audio ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return None

    except Exception as e:
        log_message(f"âŒ pyannote í™”ì ë¶„ì„ ì˜¤ë¥˜: {e}")
        return None


def smart_merge_by_speaker(segments, speaker_labels, min_duration_ms=3000):
    """
    í™”ìë³„ë¡œ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•©
    
    Args:
        segments: [(start_ms, end_ms), ...] ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        speaker_labels: [0, 0, 1, 1, 0, ...] í™”ì ë ˆì´ë¸”
        min_duration_ms: ìµœì†Œ ê¸¸ì´ (ê¸°ë³¸ 3ì´ˆ)
        
    Returns:
        merged_segments: ë³‘í•©ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        merge_map: ì›ë³¸ ì¸ë±ìŠ¤ â†’ ë³‘í•©ëœ ì¸ë±ìŠ¤ ë§¤í•‘
        merged_speaker_labels: ë³‘í•©ëœ ì„¸ê·¸ë¨¼íŠ¸ì˜ í™”ì ë ˆì´ë¸”
    """
    if len(segments) != len(speaker_labels):
        log_message("âŒ ì„¸ê·¸ë¨¼íŠ¸ì™€ í™”ì ë ˆì´ë¸” ìˆ˜ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return segments, {i: i for i in range(len(segments))}, speaker_labels

    merged_segments = []
    merged_speaker_labels = []
    merge_map = {}

    i = 0
    while i < len(segments):
        current_speaker = speaker_labels[i]
        group_segments = [segments[i]]
        group_indices = [i]

        # ê°™ì€ í™”ìì˜ ì—°ì†ëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ê·¸ë£¹í™”
        j = i + 1
        while j < len(segments) and speaker_labels[j] == current_speaker:
            group_segments.append(segments[j])
            group_indices.append(j)
            j += 1

        # ê·¸ë£¹ ë‚´ì—ì„œ 3ì´ˆ ì´ìƒ ë§Œë“¤ê¸°
        merged_group, group_merge_map = merge_group_segments(
            group_segments, group_indices, min_duration_ms
        )

        # ê²°ê³¼ì— ì¶”ê°€
        start_merged_idx = len(merged_segments)
        merged_segments.extend(merged_group)
        merged_speaker_labels.extend([current_speaker] * len(merged_group))

        # ë§¤í•‘ ì •ë³´ ì—…ë°ì´íŠ¸
        for orig_idx, local_merged_idx in group_merge_map.items():
            merge_map[orig_idx] = start_merged_idx + local_merged_idx

        log_message(f"í™”ì {current_speaker}: {len(group_segments)}ê°œ â†’ {len(merged_group)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

        i = j

    log_message(f"âœ… í™”ìë³„ ìŠ¤ë§ˆíŠ¸ ë³‘í•© ì™„ë£Œ: {len(segments)}ê°œ â†’ {len(merged_segments)}ê°œ")
    return merged_segments, merge_map, merged_speaker_labels


def merge_group_segments(group_segments, group_indices, min_duration_ms):
    """ê°™ì€ í™”ì ê·¸ë£¹ ë‚´ì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•©"""
    if not group_segments:
        return [], {}

    merged = []
    merge_map = {}
    current_group = []
    current_indices = []

    for i, (start, end) in enumerate(group_segments):
        current_group.append((start, end))
        current_indices.append(group_indices[i])

        # í˜„ì¬ ê·¸ë£¹ì˜ ì´ ê¸¸ì´
        group_start = current_group[0][0]
        group_end = current_group[-1][1]
        total_duration = group_end - group_start

        # 3ì´ˆ ì´ìƒì´ ë˜ì—ˆê±°ë‚˜ ë§ˆì§€ë§‰ì¸ ê²½ìš°
        if total_duration >= min_duration_ms or i == len(group_segments) - 1:
            merged.append((group_start, group_end))
            merged_idx = len(merged) - 1

            # ë§¤í•‘ ì •ë³´
            for orig_idx in current_indices:
                merge_map[orig_idx] = merged_idx

            # ë¦¬ì…‹
            current_group = []
            current_indices = []

    return merged, merge_map
